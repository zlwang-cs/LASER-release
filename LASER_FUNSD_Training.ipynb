{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/https://colab.research.google.com/drive/1hlkvpGma6uA_116lzDTpJeXF0ashLC6H?usp=sharing\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ],
      "metadata": {
        "id": "RqvRwDdv4JQI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our current Python version if 3.10, we need Python 3.8, lets change python version to *3.8*"
      ],
      "metadata": {
        "id": "FZC1kpV8tdGL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYHrpb2ZtVNr",
        "outputId": "dd9a8e31-702b-4b16-d8ef-ebd842931ee1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.11\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 1\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUJVCpmnt7Bm",
        "outputId": "384808fe-e32b-4c41-b786-4838deaac1ff"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "update-alternatives: using /usr/bin/python3.8 to provide /usr/bin/python3 (python3) in auto mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it should show Python 3.8"
      ],
      "metadata": {
        "id": "P_kJWhXQt-kV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIwTRkN2t8FA",
        "outputId": "400d27aa-8412-4947-f52f-faea2472dd8e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.8.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install pip and distutils"
      ],
      "metadata": {
        "id": "7vOI--j6t1d1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install python3-pip python3-distutils python3-apt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyD-YYZst03L",
        "outputId": "d44fc649-1c76-46ad-c47b-7283f0c4de80"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python3-apt is already the newest version (2.0.1ubuntu0.20.04.1).\n",
            "python3-apt set to manually installed.\n",
            "python3-distutils is already the newest version (3.8.10-0ubuntu1~20.04).\n",
            "python3-distutils set to manually installed.\n",
            "The following additional packages will be installed:\n",
            "  python-pip-whl python3-setuptools python3-wheel\n",
            "Suggested packages:\n",
            "  python-setuptools-doc\n",
            "The following NEW packages will be installed:\n",
            "  python-pip-whl python3-pip python3-setuptools python3-wheel\n",
            "0 upgraded, 4 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 2,389 kB of archives.\n",
            "After this operation, 4,933 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 python-pip-whl all 20.0.2-5ubuntu1.8 [1,805 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 python3-setuptools all 45.2.0-1ubuntu0.1 [330 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 python3-wheel all 0.34.2-1ubuntu0.1 [23.9 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 python3-pip all 20.0.2-5ubuntu1.8 [231 kB]\n",
            "Fetched 2,389 kB in 2s (1,229 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 4.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package python-pip-whl.\n",
            "(Reading database ... 122545 files and directories currently installed.)\n",
            "Preparing to unpack .../python-pip-whl_20.0.2-5ubuntu1.8_all.deb ...\n",
            "Unpacking python-pip-whl (20.0.2-5ubuntu1.8) ...\n",
            "Selecting previously unselected package python3-setuptools.\n",
            "Preparing to unpack .../python3-setuptools_45.2.0-1ubuntu0.1_all.deb ...\n",
            "Unpacking python3-setuptools (45.2.0-1ubuntu0.1) ...\n",
            "Selecting previously unselected package python3-wheel.\n",
            "Preparing to unpack .../python3-wheel_0.34.2-1ubuntu0.1_all.deb ...\n",
            "Unpacking python3-wheel (0.34.2-1ubuntu0.1) ...\n",
            "Selecting previously unselected package python3-pip.\n",
            "Preparing to unpack .../python3-pip_20.0.2-5ubuntu1.8_all.deb ...\n",
            "Unpacking python3-pip (20.0.2-5ubuntu1.8) ...\n",
            "Setting up python3-setuptools (45.2.0-1ubuntu0.1) ...\n",
            "Setting up python3-wheel (0.34.2-1ubuntu0.1) ...\n",
            "Setting up python-pip-whl (20.0.2-5ubuntu1.8) ...\n",
            "Setting up python3-pip (20.0.2-5ubuntu1.8) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone LASER code from Github"
      ],
      "metadata": {
        "id": "mGyhW3KYuNJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/zlwang-cs/LASER-release LASER"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgEgjOrDuSFZ",
        "outputId": "ba2110b8-c073-42c0-f6a4-8ea71963d643"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LASER'...\n",
            "remote: Enumerating objects: 57, done.\u001b[K\n",
            "remote: Counting objects: 100% (57/57), done.\u001b[K\n",
            "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
            "remote: Total 57 (delta 9), reused 52 (delta 7), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (57/57), 1.33 MiB | 4.54 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Pretrained Model Weights, and move them to appropriate location"
      ],
      "metadata": {
        "id": "n0pCVQhQuXG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://layoutlm.blob.core.windows.net/readingbank/model/layoutreader-base-readingbank.zip\" -O \"model.zip\"\n",
        "!unzip model.zip\n",
        "!cp pytorch_model.bin ./LASER/weights/layoutreader/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lD1Vsts5uZup",
        "outputId": "907242b3-8a9d-4b0b-e80e-483746a460d7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-30 13:05:10--  https://layoutlm.blob.core.windows.net/readingbank/model/layoutreader-base-readingbank.zip\n",
            "Resolving layoutlm.blob.core.windows.net (layoutlm.blob.core.windows.net)... 20.150.78.68\n",
            "Connecting to layoutlm.blob.core.windows.net (layoutlm.blob.core.windows.net)|20.150.78.68|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 264487184 (252M) [application/x-zip-compressed]\n",
            "Saving to: â€˜model.zipâ€™\n",
            "\n",
            "model.zip           100%[===================>] 252.23M  12.1MB/s    in 24s     \n",
            "\n",
            "2023-05-30 13:05:35 (10.5 MB/s) - â€˜model.zipâ€™ saved [264487184/264487184]\n",
            "\n",
            "Archive:  model.zip\n",
            "  inflating: config.json             \n",
            "  inflating: __MACOSX/._config.json  \n",
            "  inflating: pytorch_model.bin       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check GPU availability"
      ],
      "metadata": {
        "id": "MdLezchRufW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GE7CWo5UuoWV",
        "outputId": "9f3c6c43-502d-4f1d-9c63-88ff852b64f6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue May 30 13:05:43 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing Dependancies required. Here we are using few different versions than one which are specified in `requirements.txt`, as given versions were giving errors while training"
      ],
      "metadata": {
        "id": "4aXlB_jxuqpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LASER && \\\n",
        "pip install nltk==3.6.2 numpy==1.19.2 python_Levenshtein==0.20.9 seqeval==1.2.2 tensorboardX==2.6 tqdm==4.49.0 transformers==2.10.0  && \\\n",
        "pip install torch==1.8.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html &&\\\n",
        "mkdir -p outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lQv0GCLvOh7",
        "outputId": "1446b2d6-fc3b-4f44-d367-51a1461eec75"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nltk==3.6.2\n",
            "  Downloading nltk-3.6.2-py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.5 MB 28.1 MB/s \n",
            "\u001b[?25hCollecting numpy==1.19.2\n",
            "  Downloading numpy-1.19.2-cp38-cp38-manylinux2010_x86_64.whl (14.5 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14.5 MB 80.0 MB/s \n",
            "\u001b[?25hCollecting python_Levenshtein==0.20.9\n",
            "  Downloading python_Levenshtein-0.20.9-py3-none-any.whl (9.4 kB)\n",
            "Collecting seqeval==1.2.2\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43 kB 2.2 MB/s \n",
            "\u001b[?25hCollecting tensorboardX==2.6\n",
            "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 114 kB 69.5 MB/s \n",
            "\u001b[?25hCollecting tqdm==4.49.0\n",
            "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 69 kB 8.0 MB/s \n",
            "\u001b[?25hCollecting transformers==2.10.0\n",
            "  Downloading transformers-2.10.0-py3-none-any.whl (660 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 660 kB 74.3 MB/s \n",
            "\u001b[?25hCollecting regex\n",
            "  Downloading regex-2023.5.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 771 kB 60.6 MB/s \n",
            "\u001b[?25hCollecting click\n",
            "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96 kB 5.9 MB/s \n",
            "\u001b[?25hCollecting joblib\n",
            "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 297 kB 62.2 MB/s \n",
            "\u001b[?25hCollecting Levenshtein==0.20.9\n",
            "  Downloading Levenshtein-0.20.9-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (174 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 174 kB 83.1 MB/s \n",
            "\u001b[?25hCollecting scikit-learn>=0.21.3\n",
            "  Downloading scikit_learn-1.2.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.8 MB 64.4 MB/s \n",
            "\u001b[?25hCollecting packaging\n",
            "  Using cached packaging-23.1-py3-none-any.whl (48 kB)\n",
            "Collecting protobuf<4,>=3.8.0\n",
            "  Downloading protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.0 MB 61.5 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 880 kB 71.4 MB/s \n",
            "\u001b[?25hCollecting requests\n",
            "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62 kB 1.2 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.3 MB 61.8 MB/s \n",
            "\u001b[?25hCollecting filelock\n",
            "  Downloading filelock-3.12.0-py3-none-any.whl (10 kB)\n",
            "Collecting tokenizers==0.7.0\n",
            "  Downloading tokenizers-0.7.0-cp38-cp38-manylinux1_x86_64.whl (7.5 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.5 MB 58.9 MB/s \n",
            "\u001b[?25hCollecting rapidfuzz<3.0.0,>=2.3.0\n",
            "  Downloading rapidfuzz-2.15.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.0 MB 70.0 MB/s \n",
            "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting scipy>=1.3.2\n",
            "  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34.5 MB 20 kB/s \n",
            "\u001b[?25hCollecting six\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting charset-normalizer<4,>=2\n",
            "  Downloading charset_normalizer-3.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (195 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 195 kB 77.8 MB/s \n",
            "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
            "  Downloading urllib3-2.0.2-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 123 kB 71.9 MB/s \n",
            "\u001b[?25hCollecting idna<4,>=2.5\n",
            "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61 kB 124 kB/s \n",
            "\u001b[?25hCollecting certifi>=2017.4.17\n",
            "  Downloading certifi-2023.5.7-py3-none-any.whl (156 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 156 kB 83.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: seqeval, sacremoses\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16170 sha256=6b1f61b44394b6c700c40808f76b9b911f8fd1463fba7bb952a31f5ecdaf8f74\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/5c/ba/05fa33fa5855777b7d686e843ec07452f22a66a138e290e732\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895255 sha256=b472d16d2950361233d1e3d8eeab4c5caf27dff36a73570de0c285bf0f873e48\n",
            "  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n",
            "Successfully built seqeval sacremoses\n",
            "\u001b[31mERROR: scipy 1.10.1 has requirement numpy<1.27.0,>=1.19.5, but you'll have numpy 1.19.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tqdm, regex, click, joblib, nltk, numpy, rapidfuzz, Levenshtein, python-Levenshtein, threadpoolctl, scipy, scikit-learn, seqeval, packaging, protobuf, tensorboardX, six, sacremoses, charset-normalizer, urllib3, idna, certifi, requests, sentencepiece, filelock, tokenizers, transformers\n",
            "Successfully installed Levenshtein-0.20.9 certifi-2023.5.7 charset-normalizer-3.1.0 click-8.1.3 filelock-3.12.0 idna-3.4 joblib-1.2.0 nltk-3.6.2 numpy-1.19.2 packaging-23.1 protobuf-3.20.3 python-Levenshtein-0.20.9 rapidfuzz-2.15.1 regex-2023.5.5 requests-2.31.0 sacremoses-0.0.53 scikit-learn-1.2.2 scipy-1.10.1 sentencepiece-0.1.99 seqeval-1.2.2 six-1.16.0 tensorboardX-2.6 threadpoolctl-3.1.0 tokenizers-0.7.0 tqdm-4.49.0 transformers-2.10.0 urllib3-2.0.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.8.1+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torch-1.8.1%2Bcu111-cp38-cp38-linux_x86_64.whl (1982.2 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1982.2 MB 4.6 kB/s \n",
            "\u001b[?25hCollecting typing-extensions\n",
            "  Using cached typing_extensions-4.6.2-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torch==1.8.1+cu111) (1.19.2)\n",
            "Installing collected packages: typing-extensions, torch\n",
            "Successfully installed torch-1.8.1+cu111 typing-extensions-4.6.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "torch with cuda 11.1 is installed cause apex needed to have same Major version for installed CUDA and CUDA with which installed Pytorch was compiled.  \n",
        "Also created an `outputs` folder"
      ],
      "metadata": {
        "id": "RbBxVncKvRiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, lets install apex.  \n",
        "\n",
        "NOTE: There are some new changes which are now merged in the main branch of apex to which the LASER implementation is not compatible. So installing from `22.04-dev` branch."
      ],
      "metadata": {
        "id": "bLFlaq5hvwrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf apex && \\\n",
        "git clone https://github.com/NVIDIA/apex -b 22.04-dev"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_pzf6M9vUE9",
        "outputId": "8e03b258-1973-4716-875f-e8001a133d65"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'apex'...\n",
            "remote: Enumerating objects: 11070, done.\u001b[K\n",
            "remote: Counting objects: 100% (196/196), done.\u001b[K\n",
            "remote: Compressing objects: 100% (118/118), done.\u001b[K\n",
            "remote: Total 11070 (delta 109), reused 148 (delta 77), pack-reused 10874\u001b[K\n",
            "Receiving objects: 100% (11070/11070), 15.34 MiB | 12.48 MiB/s, done.\n",
            "Resolving deltas: 100% (7652/7652), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we start installation, there is minor change that we need to do in apex code, comment out `LINE 179` in `apex > setup.py`  \n",
        "\n",
        "It should look similar to below code: \n",
        "\n",
        "\n",
        "```python\n",
        "...\n",
        "if \"--cuda_ext\" in sys.argv:\n",
        "    sys.argv.remove(\"--cuda_ext\")\n",
        "    raise_if_cuda_home_none(\"--cuda_ext\")\n",
        "    # check_cuda_torch_binary_vs_bare_metal(CUDA_HOME)\n",
        "\n",
        "    ext_modules.append(\n",
        "        CUDAExtension(\n",
        "            name=\"amp_C\",\n",
        "            sources=[\n",
        "...\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "iMlc9XCwxXjG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now start installing apex, this takes approximately **13 mins**, so take a coffee break if you want ðŸ˜…"
      ],
      "metadata": {
        "id": "UdWSZuBrw8Iw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Takes approx. 13 mins to install\n",
        "!cd apex && \\\n",
        "python setup.py install --cpp_ext --cuda_ext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiQvUo3Ewz61",
        "outputId": "f664ef52-51ec-4a4b-e93d-52820e0cb1a9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "torch.__version__  = 1.8.1+cu111\n",
            "\n",
            "\n",
            "setup.py:111: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "  warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating apex.egg-info\n",
            "writing apex.egg-info/PKG-INFO\n",
            "writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "writing top-level names to apex.egg-info/top_level.txt\n",
            "writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/cpp_extension.py:369: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "  warnings.warn(msg.format('we could not find ninja.'))\n",
            "reading manifest file 'apex.egg-info/SOURCES.txt'\n",
            "writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib.linux-x86_64-3.8\n",
            "creating build/lib.linux-x86_64-3.8/apex\n",
            "copying apex/_autocast_utils.py -> build/lib.linux-x86_64-3.8/apex\n",
            "copying apex/__init__.py -> build/lib.linux-x86_64-3.8/apex\n",
            "creating build/lib.linux-x86_64-3.8/apex/amp\n",
            "copying apex/amp/_initialize.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "copying apex/amp/_process_optimizer.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "copying apex/amp/rnn_compat.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "copying apex/amp/compat.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "copying apex/amp/wrap.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "copying apex/amp/amp.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "copying apex/amp/handle.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "copying apex/amp/__version__.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "copying apex/amp/utils.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "copying apex/amp/_amp_state.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "copying apex/amp/opt.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "copying apex/amp/__init__.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "copying apex/amp/frontend.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "copying apex/amp/scaler.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "creating build/lib.linux-x86_64-3.8/apex/contrib\n",
            "copying apex/contrib/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib\n",
            "creating build/lib.linux-x86_64-3.8/apex/normalization\n",
            "copying apex/normalization/fused_layer_norm.py -> build/lib.linux-x86_64-3.8/apex/normalization\n",
            "copying apex/normalization/instance_norm.py -> build/lib.linux-x86_64-3.8/apex/normalization\n",
            "copying apex/normalization/__init__.py -> build/lib.linux-x86_64-3.8/apex/normalization\n",
            "creating build/lib.linux-x86_64-3.8/apex/multi_tensor_apply\n",
            "copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib.linux-x86_64-3.8/apex/multi_tensor_apply\n",
            "copying apex/multi_tensor_apply/__init__.py -> build/lib.linux-x86_64-3.8/apex/multi_tensor_apply\n",
            "creating build/lib.linux-x86_64-3.8/apex/pyprof\n",
            "copying apex/pyprof/__init__.py -> build/lib.linux-x86_64-3.8/apex/pyprof\n",
            "creating build/lib.linux-x86_64-3.8/apex/transformer\n",
            "copying apex/transformer/utils.py -> build/lib.linux-x86_64-3.8/apex/transformer\n",
            "copying apex/transformer/parallel_state.py -> build/lib.linux-x86_64-3.8/apex/transformer\n",
            "copying apex/transformer/__init__.py -> build/lib.linux-x86_64-3.8/apex/transformer\n",
            "copying apex/transformer/microbatches.py -> build/lib.linux-x86_64-3.8/apex/transformer\n",
            "copying apex/transformer/log_util.py -> build/lib.linux-x86_64-3.8/apex/transformer\n",
            "copying apex/transformer/enums.py -> build/lib.linux-x86_64-3.8/apex/transformer\n",
            "creating build/lib.linux-x86_64-3.8/apex/mlp\n",
            "copying apex/mlp/mlp.py -> build/lib.linux-x86_64-3.8/apex/mlp\n",
            "copying apex/mlp/__init__.py -> build/lib.linux-x86_64-3.8/apex/mlp\n",
            "creating build/lib.linux-x86_64-3.8/apex/reparameterization\n",
            "copying apex/reparameterization/reparameterization.py -> build/lib.linux-x86_64-3.8/apex/reparameterization\n",
            "copying apex/reparameterization/weight_norm.py -> build/lib.linux-x86_64-3.8/apex/reparameterization\n",
            "copying apex/reparameterization/__init__.py -> build/lib.linux-x86_64-3.8/apex/reparameterization\n",
            "creating build/lib.linux-x86_64-3.8/apex/parallel\n",
            "copying apex/parallel/sync_batchnorm.py -> build/lib.linux-x86_64-3.8/apex/parallel\n",
            "copying apex/parallel/multiproc.py -> build/lib.linux-x86_64-3.8/apex/parallel\n",
            "copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.8/apex/parallel\n",
            "copying apex/parallel/optimized_sync_batchnorm.py -> build/lib.linux-x86_64-3.8/apex/parallel\n",
            "copying apex/parallel/sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.8/apex/parallel\n",
            "copying apex/parallel/distributed.py -> build/lib.linux-x86_64-3.8/apex/parallel\n",
            "copying apex/parallel/__init__.py -> build/lib.linux-x86_64-3.8/apex/parallel\n",
            "copying apex/parallel/LARC.py -> build/lib.linux-x86_64-3.8/apex/parallel\n",
            "creating build/lib.linux-x86_64-3.8/apex/fp16_utils\n",
            "copying apex/fp16_utils/loss_scaler.py -> build/lib.linux-x86_64-3.8/apex/fp16_utils\n",
            "copying apex/fp16_utils/fp16util.py -> build/lib.linux-x86_64-3.8/apex/fp16_utils\n",
            "copying apex/fp16_utils/fp16_optimizer.py -> build/lib.linux-x86_64-3.8/apex/fp16_utils\n",
            "copying apex/fp16_utils/__init__.py -> build/lib.linux-x86_64-3.8/apex/fp16_utils\n",
            "creating build/lib.linux-x86_64-3.8/apex/RNN\n",
            "copying apex/RNN/RNNBackend.py -> build/lib.linux-x86_64-3.8/apex/RNN\n",
            "copying apex/RNN/models.py -> build/lib.linux-x86_64-3.8/apex/RNN\n",
            "copying apex/RNN/cells.py -> build/lib.linux-x86_64-3.8/apex/RNN\n",
            "copying apex/RNN/__init__.py -> build/lib.linux-x86_64-3.8/apex/RNN\n",
            "creating build/lib.linux-x86_64-3.8/apex/optimizers\n",
            "copying apex/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.8/apex/optimizers\n",
            "copying apex/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.8/apex/optimizers\n",
            "copying apex/optimizers/fused_novograd.py -> build/lib.linux-x86_64-3.8/apex/optimizers\n",
            "copying apex/optimizers/fused_mixed_precision_lamb.py -> build/lib.linux-x86_64-3.8/apex/optimizers\n",
            "copying apex/optimizers/fused_adagrad.py -> build/lib.linux-x86_64-3.8/apex/optimizers\n",
            "copying apex/optimizers/__init__.py -> build/lib.linux-x86_64-3.8/apex/optimizers\n",
            "copying apex/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.8/apex/optimizers\n",
            "creating build/lib.linux-x86_64-3.8/apex/fused_dense\n",
            "copying apex/fused_dense/fused_dense.py -> build/lib.linux-x86_64-3.8/apex/fused_dense\n",
            "copying apex/fused_dense/__init__.py -> build/lib.linux-x86_64-3.8/apex/fused_dense\n",
            "creating build/lib.linux-x86_64-3.8/apex/amp/lists\n",
            "copying apex/amp/lists/torch_overrides.py -> build/lib.linux-x86_64-3.8/apex/amp/lists\n",
            "copying apex/amp/lists/tensor_overrides.py -> build/lib.linux-x86_64-3.8/apex/amp/lists\n",
            "copying apex/amp/lists/__init__.py -> build/lib.linux-x86_64-3.8/apex/amp/lists\n",
            "copying apex/amp/lists/functional_overrides.py -> build/lib.linux-x86_64-3.8/apex/amp/lists\n",
            "creating build/lib.linux-x86_64-3.8/apex/contrib/xentropy\n",
            "copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib.linux-x86_64-3.8/apex/contrib/xentropy\n",
            "copying apex/contrib/xentropy/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/xentropy\n",
            "creating build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
            "copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
            "copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
            "copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
            "copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
            "copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
            "copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
            "copying apex/contrib/multihead_attn/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
            "copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
            "copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
            "copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
            "creating build/lib.linux-x86_64-3.8/apex/contrib/peer_memory\n",
            "copying apex/contrib/peer_memory/peer_memory.py -> build/lib.linux-x86_64-3.8/apex/contrib/peer_memory\n",
            "copying apex/contrib/peer_memory/peer_halo_exchange_module_tests.py -> build/lib.linux-x86_64-3.8/apex/contrib/peer_memory\n",
            "copying apex/contrib/peer_memory/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/peer_memory\n",
            "copying apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/lib.linux-x86_64-3.8/apex/contrib/peer_memory\n",
            "creating build/lib.linux-x86_64-3.8/apex/contrib/fmha\n",
            "copying apex/contrib/fmha/fmha.py -> build/lib.linux-x86_64-3.8/apex/contrib/fmha\n",
            "copying apex/contrib/fmha/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/fmha\n",
            "creating build/lib.linux-x86_64-3.8/apex/contrib/groupbn\n",
            "copying apex/contrib/groupbn/batch_norm.py -> build/lib.linux-x86_64-3.8/apex/contrib/groupbn\n",
            "copying apex/contrib/groupbn/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/groupbn\n",
            "creating build/lib.linux-x86_64-3.8/apex/contrib/sparsity\n",
            "copying apex/contrib/sparsity/sparse_masklib.py -> build/lib.linux-x86_64-3.8/apex/contrib/sparsity\n",
            "copying apex/contrib/sparsity/permutation_lib.py -> build/lib.linux-x86_64-3.8/apex/contrib/sparsity\n",
            "copying apex/contrib/sparsity/asp.py -> build/lib.linux-x86_64-3.8/apex/contrib/sparsity\n",
            "copying apex/contrib/sparsity/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/sparsity\n",
            "creating build/lib.linux-x86_64-3.8/apex/contrib/bottleneck\n",
            "copying apex/contrib/bottleneck/halo_exchangers.py -> build/lib.linux-x86_64-3.8/apex/contrib/bottleneck\n",
            "copying apex/contrib/bottleneck/bottleneck.py -> build/lib.linux-x86_64-3.8/apex/contrib/bottleneck\n",
            "copying apex/contrib/bottleneck/bottleneck_module_test.py -> build/lib.linux-x86_64-3.8/apex/contrib/bottleneck\n",
            "copying apex/contrib/bottleneck/test.py -> build/lib.linux-x86_64-3.8/apex/contrib/bottleneck\n",
            "copying apex/contrib/bottleneck/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/bottleneck\n",
            "creating build/lib.linux-x86_64-3.8/apex/contrib/layer_norm\n",
            "copying apex/contrib/layer_norm/layer_norm.py -> build/lib.linux-x86_64-3.8/apex/contrib/layer_norm\n",
            "copying apex/contrib/layer_norm/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/layer_norm\n",
            "creating build/lib.linux-x86_64-3.8/apex/contrib/optimizers\n",
            "copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib.linux-x86_64-3.8/apex/contrib/optimizers\n",
            "copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib.linux-x86_64-3.8/apex/contrib/optimizers\n",
            "copying apex/contrib/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.8/apex/contrib/optimizers\n",
            "copying apex/contrib/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.8/apex/contrib/optimizers\n",
            "copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib.linux-x86_64-3.8/apex/contrib/optimizers\n",
            "copying apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/lib.linux-x86_64-3.8/apex/contrib/optimizers\n",
            "copying apex/contrib/optimizers/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/optimizers\n",
            "copying apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/lib.linux-x86_64-3.8/apex/contrib/optimizers\n",
            "copying apex/contrib/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.8/apex/contrib/optimizers\n",
            "creating build/lib.linux-x86_64-3.8/apex/contrib/conv_bias_relu\n",
            "copying apex/contrib/conv_bias_relu/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/conv_bias_relu\n",
            "copying apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/lib.linux-x86_64-3.8/apex/contrib/conv_bias_relu\n",
            "creating build/lib.linux-x86_64-3.8/apex/contrib/focal_loss\n",
            "copying apex/contrib/focal_loss/focal_loss.py -> build/lib.linux-x86_64-3.8/apex/contrib/focal_loss\n",
            "copying apex/contrib/focal_loss/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/focal_loss\n",
            "creating build/lib.linux-x86_64-3.8/apex/contrib/transducer\n",
            "copying apex/contrib/transducer/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/transducer\n",
            "copying apex/contrib/transducer/transducer.py -> build/lib.linux-x86_64-3.8/apex/contrib/transducer\n",
            "creating build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_search_kernels\n",
            "copying apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_search_kernels\n",
            "copying apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_search_kernels\n",
            "copying apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_search_kernels\n",
            "copying apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_search_kernels\n",
            "creating build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/__main__.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/convert.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/recurrentCell.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/embedding.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/pointwise.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/linear.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/misc.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/softmax.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/randomSample.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/conv.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/index_slice_join_mutate.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/output.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/activation.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/reduction.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/dropout.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/base.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/__init__.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/data.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/blas.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/prof.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/usage.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/loss.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/pooling.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/normalization.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/utility.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/optim.py -> build/lib.linux-x86_64-3.8/apex/pyprof/prof\n",
            "creating build/lib.linux-x86_64-3.8/apex/pyprof/nvtx\n",
            "copying apex/pyprof/nvtx/nvmarker.py -> build/lib.linux-x86_64-3.8/apex/pyprof/nvtx\n",
            "copying apex/pyprof/nvtx/__init__.py -> build/lib.linux-x86_64-3.8/apex/pyprof/nvtx\n",
            "creating build/lib.linux-x86_64-3.8/apex/pyprof/parse\n",
            "copying apex/pyprof/parse/__main__.py -> build/lib.linux-x86_64-3.8/apex/pyprof/parse\n",
            "copying apex/pyprof/parse/nvvp.py -> build/lib.linux-x86_64-3.8/apex/pyprof/parse\n",
            "copying apex/pyprof/parse/db.py -> build/lib.linux-x86_64-3.8/apex/pyprof/parse\n",
            "copying apex/pyprof/parse/parse.py -> build/lib.linux-x86_64-3.8/apex/pyprof/parse\n",
            "copying apex/pyprof/parse/kernel.py -> build/lib.linux-x86_64-3.8/apex/pyprof/parse\n",
            "copying apex/pyprof/parse/__init__.py -> build/lib.linux-x86_64-3.8/apex/pyprof/parse\n",
            "creating build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel\n",
            "copying apex/transformer/pipeline_parallel/p2p_communication.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel\n",
            "copying apex/transformer/pipeline_parallel/utils.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel\n",
            "copying apex/transformer/pipeline_parallel/__init__.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel\n",
            "copying apex/transformer/pipeline_parallel/_timers.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel\n",
            "creating build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
            "copying apex/transformer/tensor_parallel/random.py -> build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
            "copying apex/transformer/tensor_parallel/mappings.py -> build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
            "copying apex/transformer/tensor_parallel/cross_entropy.py -> build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
            "copying apex/transformer/tensor_parallel/utils.py -> build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
            "copying apex/transformer/tensor_parallel/__init__.py -> build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
            "copying apex/transformer/tensor_parallel/data.py -> build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
            "copying apex/transformer/tensor_parallel/memory.py -> build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
            "copying apex/transformer/tensor_parallel/layers.py -> build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
            "creating build/lib.linux-x86_64-3.8/apex/transformer/amp\n",
            "copying apex/transformer/amp/__init__.py -> build/lib.linux-x86_64-3.8/apex/transformer/amp\n",
            "copying apex/transformer/amp/grad_scaler.py -> build/lib.linux-x86_64-3.8/apex/transformer/amp\n",
            "creating build/lib.linux-x86_64-3.8/apex/transformer/functional\n",
            "copying apex/transformer/functional/fused_softmax.py -> build/lib.linux-x86_64-3.8/apex/transformer/functional\n",
            "copying apex/transformer/functional/__init__.py -> build/lib.linux-x86_64-3.8/apex/transformer/functional\n",
            "creating build/lib.linux-x86_64-3.8/apex/transformer/_data\n",
            "copying apex/transformer/_data/_batchsampler.py -> build/lib.linux-x86_64-3.8/apex/transformer/_data\n",
            "copying apex/transformer/_data/__init__.py -> build/lib.linux-x86_64-3.8/apex/transformer/_data\n",
            "creating build/lib.linux-x86_64-3.8/apex/transformer/testing\n",
            "copying apex/transformer/testing/standalone_bert.py -> build/lib.linux-x86_64-3.8/apex/transformer/testing\n",
            "copying apex/transformer/testing/arguments.py -> build/lib.linux-x86_64-3.8/apex/transformer/testing\n",
            "copying apex/transformer/testing/standalone_gpt.py -> build/lib.linux-x86_64-3.8/apex/transformer/testing\n",
            "copying apex/transformer/testing/__init__.py -> build/lib.linux-x86_64-3.8/apex/transformer/testing\n",
            "copying apex/transformer/testing/commons.py -> build/lib.linux-x86_64-3.8/apex/transformer/testing\n",
            "copying apex/transformer/testing/global_vars.py -> build/lib.linux-x86_64-3.8/apex/transformer/testing\n",
            "copying apex/transformer/testing/distributed_test_base.py -> build/lib.linux-x86_64-3.8/apex/transformer/testing\n",
            "creating build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules\n",
            "copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules\n",
            "copying apex/transformer/pipeline_parallel/schedules/common.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules\n",
            "copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules\n",
            "copying apex/transformer/pipeline_parallel/schedules/__init__.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules\n",
            "copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules\n",
            "running build_ext\n",
            "building 'apex_C' extension\n",
            "creating build/temp.linux-x86_64-3.8\n",
            "creating build/temp.linux-x86_64-3.8/csrc\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/include/python3.8 -c csrc/flatten_unflatten.cpp -o build/temp.linux-x86_64-3.8/csrc/flatten_unflatten.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=apex_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Parallel.h:140\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/flatten_unflatten.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ParallelOpenMP.h:83:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring #pragma omp parallel [\u001b[01;35m\u001b[K-Wunknown-pragmas\u001b[m\u001b[K]\n",
            "   83 | #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "      | \n",
            "In file included from \u001b[01m\u001b[Kcsrc/flatten_unflatten.cpp:2\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/utils/tensor_flatten.h:\u001b[m\u001b[K In member function â€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& torch::utils::TensorGroup::type()\u001b[m\u001b[Kâ€™:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/utils/tensor_flatten.h:44:28:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   44 |     return tensors[0].type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K;\n",
            "      |                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/flatten_unflatten.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fwrapv -O2 -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/csrc/flatten_unflatten.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.8/apex_C.cpython-38-x86_64-linux-gnu.so\n",
            "building 'amp_C' extension\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/amp_C_frontend.cpp -o build/temp.linux-x86_64-3.8/csrc/amp_C_frontend.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Parallel.h:140\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/amp_C_frontend.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ParallelOpenMP.h:83:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring #pragma omp parallel [\u001b[01;35m\u001b[K-Wunknown-pragmas\u001b[m\u001b[K]\n",
            "   83 | #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "      | \n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_sgd_kernel.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_sgd_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_scale_kernel.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_axpby_kernel.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_l2norm_kernel.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_l2norm_kernel_mp.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_l2norm_kernel_mp.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_l2norm_scale_kernel.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_l2norm_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_lamb_stage_1.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_lamb_stage_2.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_adam.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_adam.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_adagrad.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_adagrad.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_novograd.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_novograd.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_lamb.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_lamb.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_lamb_mp.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_lamb_mp.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fwrapv -O2 -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/csrc/amp_C_frontend.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_sgd_kernel.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_scale_kernel.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_axpby_kernel.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_l2norm_kernel.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_l2norm_kernel_mp.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_l2norm_scale_kernel.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_lamb_stage_1.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_lamb_stage_2.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_adam.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_adagrad.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_novograd.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_lamb.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_lamb_mp.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/amp_C.cpython-38-x86_64-linux-gnu.so\n",
            "building 'syncbn' extension\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/syncbn.cpp -o build/temp.linux-x86_64-3.8/csrc/syncbn.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Parallel.h:140\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/syncbn.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ParallelOpenMP.h:83:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring #pragma omp parallel [\u001b[01;35m\u001b[K-Wunknown-pragmas\u001b[m\u001b[K]\n",
            "   83 | #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "      | \n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/welford.cu -o build/temp.linux-x86_64-3.8/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fwrapv -O2 -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/csrc/syncbn.o build/temp.linux-x86_64-3.8/csrc/welford.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/syncbn.cpython-38-x86_64-linux-gnu.so\n",
            "building 'fused_layer_norm_cuda' extension\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/layer_norm_cuda.cpp -o build/temp.linux-x86_64-3.8/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Parallel.h:140\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ParallelOpenMP.h:83:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring #pragma omp parallel [\u001b[01;35m\u001b[K-Wunknown-pragmas\u001b[m\u001b[K]\n",
            "   83 | #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "      | \n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:\u001b[m\u001b[K In function â€˜\u001b[01m\u001b[Kstd::vector<at::Tensor> layer_norm(at::Tensor, c10::IntArrayRef, double)\u001b[m\u001b[Kâ€™:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:145:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  145 | #define CHECK_CUDA(x) TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:173:65:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[Kâ€™\n",
            "  173 | #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "      |                                                                 \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:362:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[Kâ€™\n",
            "  362 |   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {                                 \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:145:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[Kâ€™\n",
            "  145 | #define CHECK_CUDA(x) \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:147:24:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[Kâ€™\n",
            "  147 | #define CHECK_INPUT(x) \u001b[01;36m\u001b[KCHECK_CUDA\u001b[m\u001b[K(x); CHECK_CONTIGUOUS(x)\n",
            "      |                        \u001b[01;36m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:157:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_INPUT\u001b[m\u001b[Kâ€™\n",
            "  157 |   \u001b[01;36m\u001b[KCHECK_INPUT\u001b[m\u001b[K(input);\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:\u001b[m\u001b[K In function â€˜\u001b[01m\u001b[Kstd::vector<at::Tensor> layer_norm_affine(at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)\u001b[m\u001b[Kâ€™:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:145:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  145 | #define CHECK_CUDA(x) TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:173:65:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[Kâ€™\n",
            "  173 | #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "      |                                                                 \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:362:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[Kâ€™\n",
            "  362 |   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {                                 \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:145:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[Kâ€™\n",
            "  145 | #define CHECK_CUDA(x) \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:147:24:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[Kâ€™\n",
            "  147 | #define CHECK_INPUT(x) \u001b[01;36m\u001b[KCHECK_CUDA\u001b[m\u001b[K(x); CHECK_CONTIGUOUS(x)\n",
            "      |                        \u001b[01;36m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:178:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_INPUT\u001b[m\u001b[Kâ€™\n",
            "  178 |   \u001b[01;36m\u001b[KCHECK_INPUT\u001b[m\u001b[K(input);\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:145:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  145 | #define CHECK_CUDA(x) TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:173:65:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[Kâ€™\n",
            "  173 | #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "      |                                                                 \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:362:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[Kâ€™\n",
            "  362 |   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {                                 \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:145:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[Kâ€™\n",
            "  145 | #define CHECK_CUDA(x) \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:147:24:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[Kâ€™\n",
            "  147 | #define CHECK_INPUT(x) \u001b[01;36m\u001b[KCHECK_CUDA\u001b[m\u001b[K(x); CHECK_CONTIGUOUS(x)\n",
            "      |                        \u001b[01;36m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:179:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_INPUT\u001b[m\u001b[Kâ€™\n",
            "  179 |   \u001b[01;36m\u001b[KCHECK_INPUT\u001b[m\u001b[K(gamma);\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:145:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  145 | #define CHECK_CUDA(x) TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:173:65:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[Kâ€™\n",
            "  173 | #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "      |                                                                 \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:362:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[Kâ€™\n",
            "  362 |   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {                                 \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:145:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[Kâ€™\n",
            "  145 | #define CHECK_CUDA(x) \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:147:24:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[Kâ€™\n",
            "  147 | #define CHECK_INPUT(x) \u001b[01;36m\u001b[KCHECK_CUDA\u001b[m\u001b[K(x); CHECK_CONTIGUOUS(x)\n",
            "      |                        \u001b[01;36m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:180:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_INPUT\u001b[m\u001b[Kâ€™\n",
            "  180 |   \u001b[01;36m\u001b[KCHECK_INPUT\u001b[m\u001b[K(beta);\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:\u001b[m\u001b[K In function â€˜\u001b[01m\u001b[Kstd::vector<at::Tensor> layer_norm_affine_mixed_dtypes(at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)\u001b[m\u001b[Kâ€™:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:145:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  145 | #define CHECK_CUDA(x) TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:173:65:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[Kâ€™\n",
            "  173 | #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "      |                                                                 \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:362:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[Kâ€™\n",
            "  362 |   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {                                 \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:145:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[Kâ€™\n",
            "  145 | #define CHECK_CUDA(x) \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:147:24:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[Kâ€™\n",
            "  147 | #define CHECK_INPUT(x) \u001b[01;36m\u001b[KCHECK_CUDA\u001b[m\u001b[K(x); CHECK_CONTIGUOUS(x)\n",
            "      |                        \u001b[01;36m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:202:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_INPUT\u001b[m\u001b[Kâ€™\n",
            "  202 |   \u001b[01;36m\u001b[KCHECK_INPUT\u001b[m\u001b[K(input);\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:\u001b[m\u001b[K In function â€˜\u001b[01m\u001b[Kat::Tensor layer_norm_gradient(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, double)\u001b[m\u001b[Kâ€™:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:145:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  145 | #define CHECK_CUDA(x) TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:173:65:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[Kâ€™\n",
            "  173 | #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "      |                                                                 \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:362:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[Kâ€™\n",
            "  362 |   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {                                 \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:145:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[Kâ€™\n",
            "  145 | #define CHECK_CUDA(x) \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:147:24:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[Kâ€™\n",
            "  147 | #define CHECK_INPUT(x) \u001b[01;36m\u001b[KCHECK_CUDA\u001b[m\u001b[K(x); CHECK_CONTIGUOUS(x)\n",
            "      |                        \u001b[01;36m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:244:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_INPUT\u001b[m\u001b[Kâ€™\n",
            "  244 |   \u001b[01;36m\u001b[KCHECK_INPUT\u001b[m\u001b[K(dout);\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:145:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  145 | #define CHECK_CUDA(x) TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:173:65:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[Kâ€™\n",
            "  173 | #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "      |                                                                 \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:362:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[Kâ€™\n",
            "  362 |   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {                                 \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:145:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[Kâ€™\n",
            "  145 | #define CHECK_CUDA(x) \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:147:24:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[Kâ€™\n",
            "  147 | #define CHECK_INPUT(x) \u001b[01;36m\u001b[KCHECK_CUDA\u001b[m\u001b[K(x); CHECK_CONTIGUOUS(x)\n",
            "      |                        \u001b[01;36m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:245:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_INPUT\u001b[m\u001b[Kâ€™\n",
            "  245 |   \u001b[01;36m\u001b[KCHECK_INPUT\u001b[m\u001b[K(mean);\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:145:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  145 | #define CHECK_CUDA(x) TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:173:65:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[Kâ€™\n",
            "  173 | #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "      |                                                                 \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:362:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[Kâ€™\n",
            "  362 |   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {                                 \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:145:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[Kâ€™\n",
            "  145 | #define CHECK_CUDA(x) \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:147:24:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[Kâ€™\n",
            "  147 | #define CHECK_INPUT(x) \u001b[01;36m\u001b[KCHECK_CUDA\u001b[m\u001b[K(x); CHECK_CONTIGUOUS(x)\n",
            "      |                        \u001b[01;36m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:246:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_INPUT\u001b[m\u001b[Kâ€™\n",
            "  246 |   \u001b[01;36m\u001b[KCHECK_INPUT\u001b[m\u001b[K(invvar);\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:145:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  145 | #define CHECK_CUDA(x) TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:173:65:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[Kâ€™\n",
            "  173 | #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "      |                                                                 \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:362:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[Kâ€™\n",
            "  362 |   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {                                 \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:145:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[Kâ€™\n",
            "  145 | #define CHECK_CUDA(x) \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:147:24:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[Kâ€™\n",
            "  147 | #define CHECK_INPUT(x) \u001b[01;36m\u001b[KCHECK_CUDA\u001b[m\u001b[K(x); CHECK_CONTIGUOUS(x)\n",
            "      |                        \u001b[01;36m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:247:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_INPUT\u001b[m\u001b[Kâ€™\n",
            "  247 |   \u001b[01;36m\u001b[KCHECK_INPUT\u001b[m\u001b[K(input);\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:\u001b[m\u001b[K In function â€˜\u001b[01m\u001b[Kstd::vector<at::Tensor> layer_norm_gradient_affine(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)\u001b[m\u001b[Kâ€™:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:145:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  145 | #define CHECK_CUDA(x) TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:173:65:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[Kâ€™\n",
            "  173 | #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "      |                                                                 \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:362:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[Kâ€™\n",
            "  362 |   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {                                 \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:145:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[Kâ€™\n",
            "  145 | #define CHECK_CUDA(x) \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:147:24:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[Kâ€™\n",
            "  147 | #define CHECK_INPUT(x) \u001b[01;36m\u001b[KCHECK_CUDA\u001b[m\u001b[K(x); CHECK_CONTIGUOUS(x)\n",
            "      |                        \u001b[01;36m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:270:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_INPUT\u001b[m\u001b[Kâ€™\n",
            "  270 |   \u001b[01;36m\u001b[KCHECK_INPUT\u001b[m\u001b[K(dout);\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:145:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  145 | #define CHECK_CUDA(x) TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:173:65:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[Kâ€™\n",
            "  173 | #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "      |                                                                 \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:362:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[Kâ€™\n",
            "  362 |   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {                                 \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:145:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[Kâ€™\n",
            "  145 | #define CHECK_CUDA(x) \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:147:24:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[Kâ€™\n",
            "  147 | #define CHECK_INPUT(x) \u001b[01;36m\u001b[KCHECK_CUDA\u001b[m\u001b[K(x); CHECK_CONTIGUOUS(x)\n",
            "      |                        \u001b[01;36m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:271:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_INPUT\u001b[m\u001b[Kâ€™\n",
            "  271 |   \u001b[01;36m\u001b[KCHECK_INPUT\u001b[m\u001b[K(mean);\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:145:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  145 | #define CHECK_CUDA(x) TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:173:65:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[Kâ€™\n",
            "  173 | #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "      |                                                                 \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:362:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[Kâ€™\n",
            "  362 |   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {                                 \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:145:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[Kâ€™\n",
            "  145 | #define CHECK_CUDA(x) \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:147:24:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[Kâ€™\n",
            "  147 | #define CHECK_INPUT(x) \u001b[01;36m\u001b[KCHECK_CUDA\u001b[m\u001b[K(x); CHECK_CONTIGUOUS(x)\n",
            "      |                        \u001b[01;36m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:272:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_INPUT\u001b[m\u001b[Kâ€™\n",
            "  272 |   \u001b[01;36m\u001b[KCHECK_INPUT\u001b[m\u001b[K(invvar);\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:145:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  145 | #define CHECK_CUDA(x) TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:173:65:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[Kâ€™\n",
            "  173 | #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "      |                                                                 \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:362:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[Kâ€™\n",
            "  362 |   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {                                 \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:145:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[Kâ€™\n",
            "  145 | #define CHECK_CUDA(x) \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:147:24:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[Kâ€™\n",
            "  147 | #define CHECK_INPUT(x) \u001b[01;36m\u001b[KCHECK_CUDA\u001b[m\u001b[K(x); CHECK_CONTIGUOUS(x)\n",
            "      |                        \u001b[01;36m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:273:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_INPUT\u001b[m\u001b[Kâ€™\n",
            "  273 |   \u001b[01;36m\u001b[KCHECK_INPUT\u001b[m\u001b[K(input);\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:145:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  145 | #define CHECK_CUDA(x) TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:173:65:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[Kâ€™\n",
            "  173 | #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "      |                                                                 \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:362:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[Kâ€™\n",
            "  362 |   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {                                 \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:145:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[Kâ€™\n",
            "  145 | #define CHECK_CUDA(x) \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:147:24:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[Kâ€™\n",
            "  147 | #define CHECK_INPUT(x) \u001b[01;36m\u001b[KCHECK_CUDA\u001b[m\u001b[K(x); CHECK_CONTIGUOUS(x)\n",
            "      |                        \u001b[01;36m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:274:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_INPUT\u001b[m\u001b[Kâ€™\n",
            "  274 |   \u001b[01;36m\u001b[KCHECK_INPUT\u001b[m\u001b[K(gamma);\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:145:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  145 | #define CHECK_CUDA(x) TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:173:65:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[Kâ€™\n",
            "  173 | #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "      |                                                                 \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:362:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[Kâ€™\n",
            "  362 |   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {                                 \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:145:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[Kâ€™\n",
            "  145 | #define CHECK_CUDA(x) \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:147:24:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[Kâ€™\n",
            "  147 | #define CHECK_INPUT(x) \u001b[01;36m\u001b[KCHECK_CUDA\u001b[m\u001b[K(x); CHECK_CONTIGUOUS(x)\n",
            "      |                        \u001b[01;36m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:275:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_INPUT\u001b[m\u001b[Kâ€™\n",
            "  275 |   \u001b[01;36m\u001b[KCHECK_INPUT\u001b[m\u001b[K(beta);\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:\u001b[m\u001b[K In function â€˜\u001b[01m\u001b[Kstd::vector<at::Tensor> rms_norm(at::Tensor, c10::IntArrayRef, double)\u001b[m\u001b[Kâ€™:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:301:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  301 | #define CHECK_CUDA(x) TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:173:65:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[Kâ€™\n",
            "  173 | #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "      |                                                                 \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:362:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[Kâ€™\n",
            "  362 |   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {                                 \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:301:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[Kâ€™\n",
            "  301 | #define CHECK_CUDA(x) \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:303:24:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[Kâ€™\n",
            "  303 | #define CHECK_INPUT(x) \u001b[01;36m\u001b[KCHECK_CUDA\u001b[m\u001b[K(x); CHECK_CONTIGUOUS(x)\n",
            "      |                        \u001b[01;36m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:313:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_INPUT\u001b[m\u001b[Kâ€™\n",
            "  313 |   \u001b[01;36m\u001b[KCHECK_INPUT\u001b[m\u001b[K(input);\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:\u001b[m\u001b[K In function â€˜\u001b[01m\u001b[Kstd::vector<at::Tensor> rms_norm_affine(at::Tensor, c10::IntArrayRef, at::Tensor, double)\u001b[m\u001b[Kâ€™:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:301:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  301 | #define CHECK_CUDA(x) TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:173:65:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[Kâ€™\n",
            "  173 | #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "      |                                                                 \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:362:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[Kâ€™\n",
            "  362 |   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {                                 \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:301:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[Kâ€™\n",
            "  301 | #define CHECK_CUDA(x) \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:303:24:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[Kâ€™\n",
            "  303 | #define CHECK_INPUT(x) \u001b[01;36m\u001b[KCHECK_CUDA\u001b[m\u001b[K(x); CHECK_CONTIGUOUS(x)\n",
            "      |                        \u001b[01;36m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:332:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_INPUT\u001b[m\u001b[Kâ€™\n",
            "  332 |   \u001b[01;36m\u001b[KCHECK_INPUT\u001b[m\u001b[K(input);\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:301:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  301 | #define CHECK_CUDA(x) TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:173:65:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[Kâ€™\n",
            "  173 | #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "      |                                                                 \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:362:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[Kâ€™\n",
            "  362 |   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {                                 \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:301:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[Kâ€™\n",
            "  301 | #define CHECK_CUDA(x) \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:303:24:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[Kâ€™\n",
            "  303 | #define CHECK_INPUT(x) \u001b[01;36m\u001b[KCHECK_CUDA\u001b[m\u001b[K(x); CHECK_CONTIGUOUS(x)\n",
            "      |                        \u001b[01;36m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:333:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_INPUT\u001b[m\u001b[Kâ€™\n",
            "  333 |   \u001b[01;36m\u001b[KCHECK_INPUT\u001b[m\u001b[K(gamma);\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:\u001b[m\u001b[K In function â€˜\u001b[01m\u001b[Kstd::vector<at::Tensor> rms_norm_affine_mixed_dtypes(at::Tensor, c10::IntArrayRef, at::Tensor, double)\u001b[m\u001b[Kâ€™:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:301:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  301 | #define CHECK_CUDA(x) TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:173:65:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[Kâ€™\n",
            "  173 | #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "      |                                                                 \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:362:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[Kâ€™\n",
            "  362 |   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {                                 \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:301:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[Kâ€™\n",
            "  301 | #define CHECK_CUDA(x) \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:303:24:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[Kâ€™\n",
            "  303 | #define CHECK_INPUT(x) \u001b[01;36m\u001b[KCHECK_CUDA\u001b[m\u001b[K(x); CHECK_CONTIGUOUS(x)\n",
            "      |                        \u001b[01;36m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:353:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_INPUT\u001b[m\u001b[Kâ€™\n",
            "  353 |   \u001b[01;36m\u001b[KCHECK_INPUT\u001b[m\u001b[K(input);\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:\u001b[m\u001b[K In function â€˜\u001b[01m\u001b[Kat::Tensor rms_norm_gradient(at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, double)\u001b[m\u001b[Kâ€™:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:301:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  301 | #define CHECK_CUDA(x) TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:173:65:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[Kâ€™\n",
            "  173 | #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "      |                                                                 \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:362:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[Kâ€™\n",
            "  362 |   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {                                 \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:301:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[Kâ€™\n",
            "  301 | #define CHECK_CUDA(x) \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:303:24:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[Kâ€™\n",
            "  303 | #define CHECK_INPUT(x) \u001b[01;36m\u001b[KCHECK_CUDA\u001b[m\u001b[K(x); CHECK_CONTIGUOUS(x)\n",
            "      |                        \u001b[01;36m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:390:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_INPUT\u001b[m\u001b[Kâ€™\n",
            "  390 |   \u001b[01;36m\u001b[KCHECK_INPUT\u001b[m\u001b[K(dout);\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:301:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  301 | #define CHECK_CUDA(x) TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:173:65:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[Kâ€™\n",
            "  173 | #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "      |                                                                 \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:362:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[Kâ€™\n",
            "  362 |   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {                                 \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:301:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[Kâ€™\n",
            "  301 | #define CHECK_CUDA(x) \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:303:24:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[Kâ€™\n",
            "  303 | #define CHECK_INPUT(x) \u001b[01;36m\u001b[KCHECK_CUDA\u001b[m\u001b[K(x); CHECK_CONTIGUOUS(x)\n",
            "      |                        \u001b[01;36m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:391:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_INPUT\u001b[m\u001b[Kâ€™\n",
            "  391 |   \u001b[01;36m\u001b[KCHECK_INPUT\u001b[m\u001b[K(invvar);\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:301:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  301 | #define CHECK_CUDA(x) TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:173:65:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[Kâ€™\n",
            "  173 | #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "      |                                                                 \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:362:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[Kâ€™\n",
            "  362 |   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {                                 \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:301:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[Kâ€™\n",
            "  301 | #define CHECK_CUDA(x) \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:303:24:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[Kâ€™\n",
            "  303 | #define CHECK_INPUT(x) \u001b[01;36m\u001b[KCHECK_CUDA\u001b[m\u001b[K(x); CHECK_CONTIGUOUS(x)\n",
            "      |                        \u001b[01;36m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:392:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_INPUT\u001b[m\u001b[Kâ€™\n",
            "  392 |   \u001b[01;36m\u001b[KCHECK_INPUT\u001b[m\u001b[K(input);\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:\u001b[m\u001b[K In function â€˜\u001b[01m\u001b[Kstd::vector<at::Tensor> rms_norm_gradient_affine(at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, at::Tensor, double)\u001b[m\u001b[Kâ€™:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:301:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  301 | #define CHECK_CUDA(x) TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:173:65:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[Kâ€™\n",
            "  173 | #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "      |                                                                 \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:362:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[Kâ€™\n",
            "  362 |   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {                                 \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:301:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[Kâ€™\n",
            "  301 | #define CHECK_CUDA(x) \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:303:24:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[Kâ€™\n",
            "  303 | #define CHECK_INPUT(x) \u001b[01;36m\u001b[KCHECK_CUDA\u001b[m\u001b[K(x); CHECK_CONTIGUOUS(x)\n",
            "      |                        \u001b[01;36m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:413:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_INPUT\u001b[m\u001b[Kâ€™\n",
            "  413 |   \u001b[01;36m\u001b[KCHECK_INPUT\u001b[m\u001b[K(dout);\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:301:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  301 | #define CHECK_CUDA(x) TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:173:65:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[Kâ€™\n",
            "  173 | #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "      |                                                                 \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:362:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[Kâ€™\n",
            "  362 |   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {                                 \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:301:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[Kâ€™\n",
            "  301 | #define CHECK_CUDA(x) \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:303:24:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[Kâ€™\n",
            "  303 | #define CHECK_INPUT(x) \u001b[01;36m\u001b[KCHECK_CUDA\u001b[m\u001b[K(x); CHECK_CONTIGUOUS(x)\n",
            "      |                        \u001b[01;36m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:414:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_INPUT\u001b[m\u001b[Kâ€™\n",
            "  414 |   \u001b[01;36m\u001b[KCHECK_INPUT\u001b[m\u001b[K(invvar);\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:301:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  301 | #define CHECK_CUDA(x) TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:173:65:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[Kâ€™\n",
            "  173 | #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "      |                                                                 \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:362:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[Kâ€™\n",
            "  362 |   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {                                 \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:301:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[Kâ€™\n",
            "  301 | #define CHECK_CUDA(x) \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:303:24:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[Kâ€™\n",
            "  303 | #define CHECK_INPUT(x) \u001b[01;36m\u001b[KCHECK_CUDA\u001b[m\u001b[K(x); CHECK_CONTIGUOUS(x)\n",
            "      |                        \u001b[01;36m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:415:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_INPUT\u001b[m\u001b[Kâ€™\n",
            "  415 |   \u001b[01;36m\u001b[KCHECK_INPUT\u001b[m\u001b[K(input);\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:301:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  301 | #define CHECK_CUDA(x) TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:173:65:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[Kâ€™\n",
            "  173 | #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "      |                                                                 \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:362:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[Kâ€™\n",
            "  362 |   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {                                 \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:301:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[Kâ€™\n",
            "  301 | #define CHECK_CUDA(x) \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "      |                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:303:24:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[Kâ€™\n",
            "  303 | #define CHECK_INPUT(x) \u001b[01;36m\u001b[KCHECK_CUDA\u001b[m\u001b[K(x); CHECK_CONTIGUOUS(x)\n",
            "      |                        \u001b[01;36m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:416:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_INPUT\u001b[m\u001b[Kâ€™\n",
            "  416 |   \u001b[01;36m\u001b[KCHECK_INPUT\u001b[m\u001b[K(gamma);\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/layer_norm_cuda_kernel.cu -o build/temp.linux-x86_64-3.8/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fwrapv -O2 -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/csrc/layer_norm_cuda.o build/temp.linux-x86_64-3.8/csrc/layer_norm_cuda_kernel.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/fused_layer_norm_cuda.cpython-38-x86_64-linux-gnu.so\n",
            "building 'mlp_cuda' extension\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/mlp.cpp -o build/temp.linux-x86_64-3.8/csrc/mlp.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Parallel.h:140\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/mlp.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ParallelOpenMP.h:83:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring #pragma omp parallel [\u001b[01;35m\u001b[K-Wunknown-pragmas\u001b[m\u001b[K]\n",
            "   83 | #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "      | \n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:\u001b[m\u001b[K In function â€˜\u001b[01m\u001b[Kstd::vector<at::Tensor> mlp_forward(int, int, std::vector<at::Tensor>)\u001b[m\u001b[Kâ€™:\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:57:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: â€˜\u001b[01m\u001b[Kint\u001b[m\u001b[Kâ€™ and â€˜\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "   57 |   for (int i = 0; \u001b[01;35m\u001b[Ki < num_layers\u001b[m\u001b[K; i++) {\n",
            "      |                   \u001b[01;35m\u001b[K~~^~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:64:77:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   64 |   auto out = at::empty({batch_size, output_features.back()}, inputs[0].type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K);\n",
            "      |                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/mlp.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:65:86:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   65 |   auto reserved_space = at::empty({static_cast<long>(reserved_size)}, inputs[0].type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K);\n",
            "      |                                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/mlp.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:67:59:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   67 |   auto lt_workspace = at::empty({1 << 22}, inputs[0].type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K);\n",
            "      |                                                           \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/mlp.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/mlp.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:69:54:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K, \"mlp_forward\", [&] {\n",
            "      |                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:209:28:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "  209 |     const auto& the_type = \u001b[01;36m\u001b[KTYPE\u001b[m\u001b[K;                                               \\\n",
            "      |                            \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/mlp.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/mlp.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:211:56:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[Kâ€™ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  211 |     at::ScalarType _st = ::detail::scalar_type(the_type\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                      \\\n",
            "      |                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:69:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   69 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:109:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  109 | inline at::ScalarType \u001b[01;36m\u001b[Kscalar_type\u001b[m\u001b[K(const at::DeprecatedTypeProperties& t) {\n",
            "      |                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:211:56:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[Kâ€™ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  211 |     at::ScalarType _st = ::detail::scalar_type(the_type\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                      \\\n",
            "      |                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:69:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   69 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:109:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  109 | inline at::ScalarType \u001b[01;36m\u001b[Kscalar_type\u001b[m\u001b[K(const at::DeprecatedTypeProperties& t) {\n",
            "      |                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:72:23:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: â€˜\u001b[01m\u001b[Kint\u001b[m\u001b[Kâ€™ and â€˜\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "   72 |     for (int i = 0; \u001b[01;35m\u001b[Ki < num_layers\u001b[m\u001b[K; i++) {\n",
            "      |                     \u001b[01;35m\u001b[K~~^~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:56:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE_USING_HINT\u001b[m\u001b[Kâ€™\n",
            "   56 |     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                                                        \\\n",
            "      |            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:214:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "  214 |       \u001b[01;36m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:69:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   69 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:78:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable â€˜\u001b[01m\u001b[Kresult\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "   78 |     auto \u001b[01;35m\u001b[Kresult\u001b[m\u001b[K = mlp_fp<scalar_t>(\n",
            "      |          \u001b[01;35m\u001b[K^~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:56:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE_USING_HINT\u001b[m\u001b[Kâ€™\n",
            "   56 |     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                                                        \\\n",
            "      |            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:214:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "  214 |       \u001b[01;36m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:69:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   69 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:72:23:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: â€˜\u001b[01m\u001b[Kint\u001b[m\u001b[Kâ€™ and â€˜\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "   72 |     for (int i = 0; \u001b[01;35m\u001b[Ki < num_layers\u001b[m\u001b[K; i++) {\n",
            "      |                     \u001b[01;35m\u001b[K~~^~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:56:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE_USING_HINT\u001b[m\u001b[Kâ€™\n",
            "   56 |     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                                                        \\\n",
            "      |            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:215:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "  215 |       \u001b[01;36m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:69:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   69 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:78:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable â€˜\u001b[01m\u001b[Kresult\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "   78 |     auto \u001b[01;35m\u001b[Kresult\u001b[m\u001b[K = mlp_fp<scalar_t>(\n",
            "      |          \u001b[01;35m\u001b[K^~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:56:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE_USING_HINT\u001b[m\u001b[Kâ€™\n",
            "   56 |     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                                                        \\\n",
            "      |            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:215:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "  215 |       \u001b[01;36m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:69:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   69 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:72:23:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: â€˜\u001b[01m\u001b[Kint\u001b[m\u001b[Kâ€™ and â€˜\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "   72 |     for (int i = 0; \u001b[01;35m\u001b[Ki < num_layers\u001b[m\u001b[K; i++) {\n",
            "      |                     \u001b[01;35m\u001b[K~~^~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:56:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE_USING_HINT\u001b[m\u001b[Kâ€™\n",
            "   56 |     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                                                        \\\n",
            "      |            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:216:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "  216 |       \u001b[01;36m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:69:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   69 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:78:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable â€˜\u001b[01m\u001b[Kresult\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "   78 |     auto \u001b[01;35m\u001b[Kresult\u001b[m\u001b[K = mlp_fp<scalar_t>(\n",
            "      |          \u001b[01;35m\u001b[K^~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:56:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE_USING_HINT\u001b[m\u001b[Kâ€™\n",
            "   56 |     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                                                        \\\n",
            "      |            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:216:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "  216 |       \u001b[01;36m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:69:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   69 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:\u001b[m\u001b[K In function â€˜\u001b[01m\u001b[Kstd::vector<at::Tensor> mlp_backward(int, int, at::Tensor, std::vector<at::Tensor>, std::vector<at::Tensor>)\u001b[m\u001b[Kâ€™:\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:115:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: â€˜\u001b[01m\u001b[Kint\u001b[m\u001b[Kâ€™ and â€˜\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "  115 |   for (int i = 0; \u001b[01;35m\u001b[Ki < num_layers\u001b[m\u001b[K; i++) {\n",
            "      |                   \u001b[01;35m\u001b[K~~^~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:120:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: â€˜\u001b[01m\u001b[Kint\u001b[m\u001b[Kâ€™ and â€˜\u001b[01m\u001b[Kstd::vector<at::Tensor>::size_type\u001b[m\u001b[Kâ€™ {aka â€˜\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[Kâ€™} [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "  120 |   for (int i = 0; \u001b[01;35m\u001b[Ki < inputs.size()\u001b[m\u001b[K; i++) {\n",
            "      |                   \u001b[01;35m\u001b[K~~^~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:121:67:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  121 |     outputs.push_back(at::empty(inputs[i].sizes(), inputs[i].type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K));  // clone for testing now\n",
            "      |                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/mlp.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/mlp.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:124:54:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K, \"mlp_backward\", [&] {\n",
            "      |                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:209:28:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "  209 |     const auto& the_type = \u001b[01;36m\u001b[KTYPE\u001b[m\u001b[K;                                               \\\n",
            "      |                            \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/mlp.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/mlp.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:211:56:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[Kâ€™ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  211 |     at::ScalarType _st = ::detail::scalar_type(the_type\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                      \\\n",
            "      |                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:124:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "  124 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:109:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  109 | inline at::ScalarType \u001b[01;36m\u001b[Kscalar_type\u001b[m\u001b[K(const at::DeprecatedTypeProperties& t) {\n",
            "      |                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:211:56:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[Kâ€™ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  211 |     at::ScalarType _st = ::detail::scalar_type(the_type\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                      \\\n",
            "      |                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:124:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "  124 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:109:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  109 | inline at::ScalarType \u001b[01;36m\u001b[Kscalar_type\u001b[m\u001b[K(const at::DeprecatedTypeProperties& t) {\n",
            "      |                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:126:23:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: â€˜\u001b[01m\u001b[Kint\u001b[m\u001b[Kâ€™ and â€˜\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "  126 |     for (int i = 0; \u001b[01;35m\u001b[Ki < num_layers\u001b[m\u001b[K; i++) {\n",
            "      |                     \u001b[01;35m\u001b[K~~^~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:56:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE_USING_HINT\u001b[m\u001b[Kâ€™\n",
            "   56 |     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                                                        \\\n",
            "      |            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:214:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "  214 |       \u001b[01;36m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:124:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "  124 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:130:23:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: â€˜\u001b[01m\u001b[Kint\u001b[m\u001b[Kâ€™ and â€˜\u001b[01m\u001b[Kstd::vector<at::Tensor>::size_type\u001b[m\u001b[Kâ€™ {aka â€˜\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[Kâ€™} [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "  130 |     for (int i = 0; \u001b[01;35m\u001b[Ki < inputs.size()\u001b[m\u001b[K; i++) {\n",
            "      |                     \u001b[01;35m\u001b[K~~^~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:56:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE_USING_HINT\u001b[m\u001b[Kâ€™\n",
            "   56 |     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                                                        \\\n",
            "      |            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:214:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "  214 |       \u001b[01;36m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:124:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "  124 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:138:99:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  138 |     auto work_space = at::empty({static_cast<long>(work_size / sizeof(scalar_t))}, inputs[0].type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K);\n",
            "      |                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:56:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE_USING_HINT\u001b[m\u001b[Kâ€™\n",
            "   56 |     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                                                        \\\n",
            "      |            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:214:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "  214 |       \u001b[01;36m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:124:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "  124 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/mlp.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/mlp.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:140:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable â€˜\u001b[01m\u001b[Kresult\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "  140 |     auto \u001b[01;35m\u001b[Kresult\u001b[m\u001b[K = mlp_bp<scalar_t>(\n",
            "      |          \u001b[01;35m\u001b[K^~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:56:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE_USING_HINT\u001b[m\u001b[Kâ€™\n",
            "   56 |     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                                                        \\\n",
            "      |            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:214:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "  214 |       \u001b[01;36m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:124:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "  124 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:126:23:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: â€˜\u001b[01m\u001b[Kint\u001b[m\u001b[Kâ€™ and â€˜\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "  126 |     for (int i = 0; \u001b[01;35m\u001b[Ki < num_layers\u001b[m\u001b[K; i++) {\n",
            "      |                     \u001b[01;35m\u001b[K~~^~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:56:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE_USING_HINT\u001b[m\u001b[Kâ€™\n",
            "   56 |     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                                                        \\\n",
            "      |            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:215:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "  215 |       \u001b[01;36m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:124:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "  124 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:130:23:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: â€˜\u001b[01m\u001b[Kint\u001b[m\u001b[Kâ€™ and â€˜\u001b[01m\u001b[Kstd::vector<at::Tensor>::size_type\u001b[m\u001b[Kâ€™ {aka â€˜\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[Kâ€™} [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "  130 |     for (int i = 0; \u001b[01;35m\u001b[Ki < inputs.size()\u001b[m\u001b[K; i++) {\n",
            "      |                     \u001b[01;35m\u001b[K~~^~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:56:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE_USING_HINT\u001b[m\u001b[Kâ€™\n",
            "   56 |     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                                                        \\\n",
            "      |            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:215:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "  215 |       \u001b[01;36m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:124:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "  124 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:138:99:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  138 |     auto work_space = at::empty({static_cast<long>(work_size / sizeof(scalar_t))}, inputs[0].type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K);\n",
            "      |                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:56:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE_USING_HINT\u001b[m\u001b[Kâ€™\n",
            "   56 |     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                                                        \\\n",
            "      |            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:215:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "  215 |       \u001b[01;36m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:124:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "  124 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/mlp.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/mlp.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:140:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable â€˜\u001b[01m\u001b[Kresult\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "  140 |     auto \u001b[01;35m\u001b[Kresult\u001b[m\u001b[K = mlp_bp<scalar_t>(\n",
            "      |          \u001b[01;35m\u001b[K^~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:56:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE_USING_HINT\u001b[m\u001b[Kâ€™\n",
            "   56 |     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                                                        \\\n",
            "      |            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:215:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "  215 |       \u001b[01;36m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:124:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "  124 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:126:23:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: â€˜\u001b[01m\u001b[Kint\u001b[m\u001b[Kâ€™ and â€˜\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "  126 |     for (int i = 0; \u001b[01;35m\u001b[Ki < num_layers\u001b[m\u001b[K; i++) {\n",
            "      |                     \u001b[01;35m\u001b[K~~^~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:56:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE_USING_HINT\u001b[m\u001b[Kâ€™\n",
            "   56 |     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                                                        \\\n",
            "      |            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:216:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "  216 |       \u001b[01;36m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:124:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "  124 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:130:23:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: â€˜\u001b[01m\u001b[Kint\u001b[m\u001b[Kâ€™ and â€˜\u001b[01m\u001b[Kstd::vector<at::Tensor>::size_type\u001b[m\u001b[Kâ€™ {aka â€˜\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[Kâ€™} [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "  130 |     for (int i = 0; \u001b[01;35m\u001b[Ki < inputs.size()\u001b[m\u001b[K; i++) {\n",
            "      |                     \u001b[01;35m\u001b[K~~^~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:56:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE_USING_HINT\u001b[m\u001b[Kâ€™\n",
            "   56 |     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                                                        \\\n",
            "      |            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:216:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "  216 |       \u001b[01;36m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:124:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "  124 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:138:99:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  138 |     auto work_space = at::empty({static_cast<long>(work_size / sizeof(scalar_t))}, inputs[0].type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K);\n",
            "      |                                                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:56:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE_USING_HINT\u001b[m\u001b[Kâ€™\n",
            "   56 |     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                                                        \\\n",
            "      |            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:216:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "  216 |       \u001b[01;36m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:124:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "  124 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/mlp.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/mlp.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:140:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable â€˜\u001b[01m\u001b[Kresult\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "  140 |     auto \u001b[01;35m\u001b[Kresult\u001b[m\u001b[K = mlp_bp<scalar_t>(\n",
            "      |          \u001b[01;35m\u001b[K^~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:56:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE_USING_HINT\u001b[m\u001b[Kâ€™\n",
            "   56 |     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                                                        \\\n",
            "      |            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:216:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "  216 |       \u001b[01;36m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:124:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "  124 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/mlp_cuda.cu -o build/temp.linux-x86_64-3.8/csrc/mlp_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(110): warning #20236-D: pragma \"diag_suppress\" is deprecated, use \"nv_diag_suppress\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(117): warning #20236-D: pragma \"diag_default\" is deprecated, use \"nv_diag_default\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(110): warning #20236-D: pragma \"diag_suppress\" is deprecated, use \"nv_diag_suppress\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(117): warning #20236-D: pragma \"diag_default\" is deprecated, use \"nv_diag_default\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(110): warning #20236-D: pragma \"diag_suppress\" is deprecated, use \"nv_diag_suppress\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(117): warning #20236-D: pragma \"diag_default\" is deprecated, use \"nv_diag_default\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(110): warning #20236-D: pragma \"diag_suppress\" is deprecated, use \"nv_diag_suppress\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(117): warning #20236-D: pragma \"diag_default\" is deprecated, use \"nv_diag_default\" instead\n",
            "\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fwrapv -O2 -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/csrc/mlp.o build/temp.linux-x86_64-3.8/csrc/mlp_cuda.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/mlp_cuda.cpython-38-x86_64-linux-gnu.so\n",
            "building 'fused_dense_cuda' extension\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/fused_dense.cpp -o build/temp.linux-x86_64-3.8/csrc/fused_dense.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_dense_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Parallel.h:140\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/fused_dense.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ParallelOpenMP.h:83:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring #pragma omp parallel [\u001b[01;35m\u001b[K-Wunknown-pragmas\u001b[m\u001b[K]\n",
            "   83 | #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "      | \n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:\u001b[m\u001b[K In function â€˜\u001b[01m\u001b[Kat::Tensor linear_bias_forward(at::Tensor, at::Tensor, at::Tensor)\u001b[m\u001b[Kâ€™:\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:30:63:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   30 |   auto out = at::empty({batch_size, out_features}, input.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K);\n",
            "      |                                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/fused_dense.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:33:55:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   33 |   auto lt_workspace = at::empty({1 << 22}, input.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K);\n",
            "      |                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/fused_dense.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/fused_dense.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:35:50:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   35 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K, \"linear_bias_forward\", [&] {\n",
            "      |                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:209:28:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "  209 |     const auto& the_type = \u001b[01;36m\u001b[KTYPE\u001b[m\u001b[K;                                               \\\n",
            "      |                            \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/fused_dense.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/fused_dense.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:211:56:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[Kâ€™ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  211 |     at::ScalarType _st = ::detail::scalar_type(the_type\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                      \\\n",
            "      |                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:35:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   35 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(input.type(), \"linear_bias_forward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:109:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  109 | inline at::ScalarType \u001b[01;36m\u001b[Kscalar_type\u001b[m\u001b[K(const at::DeprecatedTypeProperties& t) {\n",
            "      |                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:211:56:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[Kâ€™ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  211 |     at::ScalarType _st = ::detail::scalar_type(the_type\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                      \\\n",
            "      |                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:35:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   35 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(input.type(), \"linear_bias_forward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:109:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  109 | inline at::ScalarType \u001b[01;36m\u001b[Kscalar_type\u001b[m\u001b[K(const at::DeprecatedTypeProperties& t) {\n",
            "      |                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:37:15:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable â€˜\u001b[01m\u001b[Kb_ptr\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "   37 |     scalar_t* \u001b[01;35m\u001b[Kb_ptr\u001b[m\u001b[K = bias.data_ptr<scalar_t>();\n",
            "      |               \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:56:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE_USING_HINT\u001b[m\u001b[Kâ€™\n",
            "   56 |     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                                                        \\\n",
            "      |            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:214:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "  214 |       \u001b[01;36m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:35:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   35 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(input.type(), \"linear_bias_forward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:38:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable â€˜\u001b[01m\u001b[Kresult\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "   38 |     auto \u001b[01;35m\u001b[Kresult\u001b[m\u001b[K = linear_bias_forward_cuda<scalar_t>(\n",
            "      |          \u001b[01;35m\u001b[K^~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:56:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE_USING_HINT\u001b[m\u001b[Kâ€™\n",
            "   56 |     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                                                        \\\n",
            "      |            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:214:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "  214 |       \u001b[01;36m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:35:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   35 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(input.type(), \"linear_bias_forward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:37:15:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable â€˜\u001b[01m\u001b[Kb_ptr\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "   37 |     scalar_t* \u001b[01;35m\u001b[Kb_ptr\u001b[m\u001b[K = bias.data_ptr<scalar_t>();\n",
            "      |               \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:56:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE_USING_HINT\u001b[m\u001b[Kâ€™\n",
            "   56 |     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                                                        \\\n",
            "      |            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:215:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "  215 |       \u001b[01;36m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:35:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   35 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(input.type(), \"linear_bias_forward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:38:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable â€˜\u001b[01m\u001b[Kresult\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "   38 |     auto \u001b[01;35m\u001b[Kresult\u001b[m\u001b[K = linear_bias_forward_cuda<scalar_t>(\n",
            "      |          \u001b[01;35m\u001b[K^~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:56:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE_USING_HINT\u001b[m\u001b[Kâ€™\n",
            "   56 |     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                                                        \\\n",
            "      |            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:215:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "  215 |       \u001b[01;36m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:35:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   35 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(input.type(), \"linear_bias_forward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:37:15:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable â€˜\u001b[01m\u001b[Kb_ptr\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "   37 |     scalar_t* \u001b[01;35m\u001b[Kb_ptr\u001b[m\u001b[K = bias.data_ptr<scalar_t>();\n",
            "      |               \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:56:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE_USING_HINT\u001b[m\u001b[Kâ€™\n",
            "   56 |     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                                                        \\\n",
            "      |            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:216:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "  216 |       \u001b[01;36m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:35:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   35 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(input.type(), \"linear_bias_forward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:38:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable â€˜\u001b[01m\u001b[Kresult\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "   38 |     auto \u001b[01;35m\u001b[Kresult\u001b[m\u001b[K = linear_bias_forward_cuda<scalar_t>(\n",
            "      |          \u001b[01;35m\u001b[K^~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:56:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE_USING_HINT\u001b[m\u001b[Kâ€™\n",
            "   56 |     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                                                        \\\n",
            "      |            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:216:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "  216 |       \u001b[01;36m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:35:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   35 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(input.type(), \"linear_bias_forward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:\u001b[m\u001b[K In function â€˜\u001b[01m\u001b[Kstd::vector<at::Tensor> linear_bias_backward(at::Tensor, at::Tensor, at::Tensor)\u001b[m\u001b[Kâ€™:\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:64:69:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   64 |   auto d_weight = at::empty({out_features, in_features}, input.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K);\n",
            "      |                                                                     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/fused_dense.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:68:54:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   68 |   auto d_bias = at::empty({out_features}, input.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K);\n",
            "      |                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/fused_dense.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:70:66:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   70 |   auto d_input = at::empty({batch_size, in_features}, input.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K);\n",
            "      |                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/fused_dense.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:73:55:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   73 |   auto lt_workspace = at::empty({1 << 22}, input.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K);\n",
            "      |                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/fused_dense.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/fused_dense.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:75:50:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   75 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K, \"linear_bias_backward\", [&] {\n",
            "      |                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:209:28:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "  209 |     const auto& the_type = \u001b[01;36m\u001b[KTYPE\u001b[m\u001b[K;                                               \\\n",
            "      |                            \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/fused_dense.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/fused_dense.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:211:56:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[Kâ€™ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  211 |     at::ScalarType _st = ::detail::scalar_type(the_type\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                      \\\n",
            "      |                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:75:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   75 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(input.type(), \"linear_bias_backward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:109:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  109 | inline at::ScalarType \u001b[01;36m\u001b[Kscalar_type\u001b[m\u001b[K(const at::DeprecatedTypeProperties& t) {\n",
            "      |                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:211:56:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[Kâ€™ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  211 |     at::ScalarType _st = ::detail::scalar_type(the_type\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                      \\\n",
            "      |                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:75:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   75 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(input.type(), \"linear_bias_backward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:109:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  109 | inline at::ScalarType \u001b[01;36m\u001b[Kscalar_type\u001b[m\u001b[K(const at::DeprecatedTypeProperties& t) {\n",
            "      |                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:77:15:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable â€˜\u001b[01m\u001b[Kd_b_ptr\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "   77 |     scalar_t* \u001b[01;35m\u001b[Kd_b_ptr\u001b[m\u001b[K = d_bias.data_ptr<scalar_t>();\n",
            "      |               \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:56:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE_USING_HINT\u001b[m\u001b[Kâ€™\n",
            "   56 |     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                                                        \\\n",
            "      |            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:214:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "  214 |       \u001b[01;36m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:75:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   75 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(input.type(), \"linear_bias_backward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:78:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable â€˜\u001b[01m\u001b[Kresult\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "   78 |     auto \u001b[01;35m\u001b[Kresult\u001b[m\u001b[K = linear_bias_backward_cuda<scalar_t>(\n",
            "      |          \u001b[01;35m\u001b[K^~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:56:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE_USING_HINT\u001b[m\u001b[Kâ€™\n",
            "   56 |     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                                                        \\\n",
            "      |            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:214:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "  214 |       \u001b[01;36m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:75:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   75 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(input.type(), \"linear_bias_backward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:77:15:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable â€˜\u001b[01m\u001b[Kd_b_ptr\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "   77 |     scalar_t* \u001b[01;35m\u001b[Kd_b_ptr\u001b[m\u001b[K = d_bias.data_ptr<scalar_t>();\n",
            "      |               \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:56:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE_USING_HINT\u001b[m\u001b[Kâ€™\n",
            "   56 |     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                                                        \\\n",
            "      |            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:215:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "  215 |       \u001b[01;36m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:75:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   75 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(input.type(), \"linear_bias_backward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:78:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable â€˜\u001b[01m\u001b[Kresult\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "   78 |     auto \u001b[01;35m\u001b[Kresult\u001b[m\u001b[K = linear_bias_backward_cuda<scalar_t>(\n",
            "      |          \u001b[01;35m\u001b[K^~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:56:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE_USING_HINT\u001b[m\u001b[Kâ€™\n",
            "   56 |     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                                                        \\\n",
            "      |            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:215:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "  215 |       \u001b[01;36m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:75:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   75 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(input.type(), \"linear_bias_backward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:77:15:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable â€˜\u001b[01m\u001b[Kd_b_ptr\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "   77 |     scalar_t* \u001b[01;35m\u001b[Kd_b_ptr\u001b[m\u001b[K = d_bias.data_ptr<scalar_t>();\n",
            "      |               \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:56:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE_USING_HINT\u001b[m\u001b[Kâ€™\n",
            "   56 |     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                                                        \\\n",
            "      |            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:216:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "  216 |       \u001b[01;36m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:75:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   75 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(input.type(), \"linear_bias_backward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:78:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable â€˜\u001b[01m\u001b[Kresult\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "   78 |     auto \u001b[01;35m\u001b[Kresult\u001b[m\u001b[K = linear_bias_backward_cuda<scalar_t>(\n",
            "      |          \u001b[01;35m\u001b[K^~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:56:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE_USING_HINT\u001b[m\u001b[Kâ€™\n",
            "   56 |     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                                                        \\\n",
            "      |            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:216:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "  216 |       \u001b[01;36m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:75:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   75 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(input.type(), \"linear_bias_backward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:\u001b[m\u001b[K In function â€˜\u001b[01m\u001b[Kstd::vector<at::Tensor> linear_gelu_linear_forward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor)\u001b[m\u001b[Kâ€™:\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:106:70:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  106 |   auto output1 = at::empty({batch_size, hidden_features}, input.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K);\n",
            "      |                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/fused_dense.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:107:70:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  107 |   auto gelu_in = at::empty({batch_size, hidden_features}, input.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K);\n",
            "      |                                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/fused_dense.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:108:67:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  108 |   auto output2 = at::empty({batch_size, out_features}, input.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K);\n",
            "      |                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/fused_dense.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:111:55:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  111 |   auto lt_workspace = at::empty({1 << 22}, input.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K);\n",
            "      |                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/fused_dense.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/fused_dense.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:113:50:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  113 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K, \"linear_gelu_linear_forward\", [&] {\n",
            "      |                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:209:28:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "  209 |     const auto& the_type = \u001b[01;36m\u001b[KTYPE\u001b[m\u001b[K;                                               \\\n",
            "      |                            \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/fused_dense.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/fused_dense.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:211:56:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[Kâ€™ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  211 |     at::ScalarType _st = ::detail::scalar_type(the_type\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                      \\\n",
            "      |                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:113:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "  113 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(input.type(), \"linear_gelu_linear_forward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:109:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  109 | inline at::ScalarType \u001b[01;36m\u001b[Kscalar_type\u001b[m\u001b[K(const at::DeprecatedTypeProperties& t) {\n",
            "      |                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:211:56:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[Kâ€™ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  211 |     at::ScalarType _st = ::detail::scalar_type(the_type\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                      \\\n",
            "      |                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:113:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "  113 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(input.type(), \"linear_gelu_linear_forward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:109:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  109 | inline at::ScalarType \u001b[01;36m\u001b[Kscalar_type\u001b[m\u001b[K(const at::DeprecatedTypeProperties& t) {\n",
            "      |                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:118:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable â€˜\u001b[01m\u001b[Kresult\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "  118 |     auto \u001b[01;35m\u001b[Kresult\u001b[m\u001b[K = linear_gelu_linear_forward_cuda<scalar_t>(\n",
            "      |          \u001b[01;35m\u001b[K^~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:56:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE_USING_HINT\u001b[m\u001b[Kâ€™\n",
            "   56 |     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                                                        \\\n",
            "      |            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:214:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "  214 |       \u001b[01;36m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:113:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "  113 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(input.type(), \"linear_gelu_linear_forward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:118:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable â€˜\u001b[01m\u001b[Kresult\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "  118 |     auto \u001b[01;35m\u001b[Kresult\u001b[m\u001b[K = linear_gelu_linear_forward_cuda<scalar_t>(\n",
            "      |          \u001b[01;35m\u001b[K^~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:56:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE_USING_HINT\u001b[m\u001b[Kâ€™\n",
            "   56 |     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                                                        \\\n",
            "      |            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:215:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "  215 |       \u001b[01;36m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:113:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "  113 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(input.type(), \"linear_gelu_linear_forward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:118:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable â€˜\u001b[01m\u001b[Kresult\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "  118 |     auto \u001b[01;35m\u001b[Kresult\u001b[m\u001b[K = linear_gelu_linear_forward_cuda<scalar_t>(\n",
            "      |          \u001b[01;35m\u001b[K^~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:56:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE_USING_HINT\u001b[m\u001b[Kâ€™\n",
            "   56 |     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                                                        \\\n",
            "      |            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:216:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "  216 |       \u001b[01;36m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:113:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "  113 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(input.type(), \"linear_gelu_linear_forward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:\u001b[m\u001b[K In function â€˜\u001b[01m\u001b[Kstd::vector<at::Tensor> linear_gelu_linear_backward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor)\u001b[m\u001b[Kâ€™:\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:149:73:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  149 |   auto d_weight1 = at::empty({hidden_features, in_features}, input.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K);\n",
            "      |                                                                         \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/fused_dense.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:150:74:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  150 |   auto d_weight2 = at::empty({out_features, hidden_features}, input.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K);\n",
            "      |                                                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/fused_dense.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:151:58:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  151 |   auto d_bias1 = at::empty({hidden_features}, input.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K);\n",
            "      |                                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/fused_dense.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:152:55:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  152 |   auto d_bias2 = at::empty({out_features}, input.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K);\n",
            "      |                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/fused_dense.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:153:66:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  153 |   auto d_input = at::empty({batch_size, in_features}, input.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K);\n",
            "      |                                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/fused_dense.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:154:72:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  154 |   auto d_output1 = at::empty({batch_size, hidden_features}, input.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K);\n",
            "      |                                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/fused_dense.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:157:55:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  157 |   auto lt_workspace = at::empty({1 << 22}, input.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K);\n",
            "      |                                                       \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/fused_dense.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/fused_dense.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:159:50:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  159 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K, \"linear_bias_backward\", [&] {\n",
            "      |                                                  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:209:28:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "  209 |     const auto& the_type = \u001b[01;36m\u001b[KTYPE\u001b[m\u001b[K;                                               \\\n",
            "      |                            \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:9\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/fused_dense.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:303:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  303 |   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "      |                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/fused_dense.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:211:56:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[Kâ€™ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  211 |     at::ScalarType _st = ::detail::scalar_type(the_type\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                      \\\n",
            "      |                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:159:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "  159 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(input.type(), \"linear_bias_backward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:109:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  109 | inline at::ScalarType \u001b[01;36m\u001b[Kscalar_type\u001b[m\u001b[K(const at::DeprecatedTypeProperties& t) {\n",
            "      |                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:211:56:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[Kâ€™ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "  211 |     at::ScalarType _st = ::detail::scalar_type(the_type\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                      \\\n",
            "      |                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:159:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "  159 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(input.type(), \"linear_bias_backward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:109:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "  109 | inline at::ScalarType \u001b[01;36m\u001b[Kscalar_type\u001b[m\u001b[K(const at::DeprecatedTypeProperties& t) {\n",
            "      |                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:162:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable â€˜\u001b[01m\u001b[Kresult\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "  162 |     auto \u001b[01;35m\u001b[Kresult\u001b[m\u001b[K = linear_gelu_linear_backward_cuda<scalar_t>(\n",
            "      |          \u001b[01;35m\u001b[K^~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:56:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE_USING_HINT\u001b[m\u001b[Kâ€™\n",
            "   56 |     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                                                        \\\n",
            "      |            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:214:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "  214 |       \u001b[01;36m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:159:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "  159 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(input.type(), \"linear_bias_backward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:162:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable â€˜\u001b[01m\u001b[Kresult\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "  162 |     auto \u001b[01;35m\u001b[Kresult\u001b[m\u001b[K = linear_gelu_linear_backward_cuda<scalar_t>(\n",
            "      |          \u001b[01;35m\u001b[K^~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:56:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE_USING_HINT\u001b[m\u001b[Kâ€™\n",
            "   56 |     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                                                        \\\n",
            "      |            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:215:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "  215 |       \u001b[01;36m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:159:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "  159 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(input.type(), \"linear_bias_backward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:162:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable â€˜\u001b[01m\u001b[Kresult\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "  162 |     auto \u001b[01;35m\u001b[Kresult\u001b[m\u001b[K = linear_gelu_linear_backward_cuda<scalar_t>(\n",
            "      |          \u001b[01;35m\u001b[K^~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:56:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE_USING_HINT\u001b[m\u001b[Kâ€™\n",
            "   56 |     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                                                        \\\n",
            "      |            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:216:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "  216 |       \u001b[01;36m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "      |       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/fused_dense.cpp:159:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "  159 |   \u001b[01;36m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K(input.type(), \"linear_bias_backward\", [&] {\n",
            "      |   \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/fused_dense_cuda.cu -o build/temp.linux-x86_64-3.8/csrc/fused_dense_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_dense_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(110): warning #20236-D: pragma \"diag_suppress\" is deprecated, use \"nv_diag_suppress\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(117): warning #20236-D: pragma \"diag_default\" is deprecated, use \"nv_diag_default\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(110): warning #20236-D: pragma \"diag_suppress\" is deprecated, use \"nv_diag_suppress\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(117): warning #20236-D: pragma \"diag_default\" is deprecated, use \"nv_diag_default\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(110): warning #20236-D: pragma \"diag_suppress\" is deprecated, use \"nv_diag_suppress\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(117): warning #20236-D: pragma \"diag_default\" is deprecated, use \"nv_diag_default\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(110): warning #20236-D: pragma \"diag_suppress\" is deprecated, use \"nv_diag_suppress\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(117): warning #20236-D: pragma \"diag_default\" is deprecated, use \"nv_diag_default\" instead\n",
            "\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fwrapv -O2 -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/csrc/fused_dense.o build/temp.linux-x86_64-3.8/csrc/fused_dense_cuda.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/fused_dense_cuda.cpython-38-x86_64-linux-gnu.so\n",
            "building 'scaled_upper_triang_masked_softmax_cuda' extension\n",
            "creating build/temp.linux-x86_64-3.8/csrc/megatron\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/content/apex/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/megatron/scaled_upper_triang_masked_softmax.cpp -o build/temp.linux-x86_64-3.8/csrc/megatron/scaled_upper_triang_masked_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Parallel.h:140\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/megatron/scaled_upper_triang_masked_softmax.cpp:18\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ParallelOpenMP.h:83:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring #pragma omp parallel [\u001b[01;35m\u001b[K-Wunknown-pragmas\u001b[m\u001b[K]\n",
            "   83 | #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "      | \n",
            "/usr/local/cuda/bin/nvcc -I/content/apex/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/megatron/scaled_upper_triang_masked_softmax_cuda.cu -o build/temp.linux-x86_64-3.8/csrc/megatron/scaled_upper_triang_masked_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(110): warning #20236-D: pragma \"diag_suppress\" is deprecated, use \"nv_diag_suppress\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(117): warning #20236-D: pragma \"diag_default\" is deprecated, use \"nv_diag_default\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(110): warning #20236-D: pragma \"diag_suppress\" is deprecated, use \"nv_diag_suppress\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(117): warning #20236-D: pragma \"diag_default\" is deprecated, use \"nv_diag_default\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(110): warning #20236-D: pragma \"diag_suppress\" is deprecated, use \"nv_diag_suppress\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(117): warning #20236-D: pragma \"diag_default\" is deprecated, use \"nv_diag_default\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(110): warning #20236-D: pragma \"diag_suppress\" is deprecated, use \"nv_diag_suppress\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(117): warning #20236-D: pragma \"diag_default\" is deprecated, use \"nv_diag_default\" instead\n",
            "\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fwrapv -O2 -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/csrc/megatron/scaled_upper_triang_masked_softmax.o build/temp.linux-x86_64-3.8/csrc/megatron/scaled_upper_triang_masked_softmax_cuda.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/scaled_upper_triang_masked_softmax_cuda.cpython-38-x86_64-linux-gnu.so\n",
            "building 'scaled_masked_softmax_cuda' extension\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/content/apex/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/megatron/scaled_masked_softmax.cpp -o build/temp.linux-x86_64-3.8/csrc/megatron/scaled_masked_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Parallel.h:140\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/megatron/scaled_masked_softmax.cpp:18\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ParallelOpenMP.h:83:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring #pragma omp parallel [\u001b[01;35m\u001b[K-Wunknown-pragmas\u001b[m\u001b[K]\n",
            "   83 | #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "      | \n",
            "/usr/local/cuda/bin/nvcc -I/content/apex/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/megatron/scaled_masked_softmax_cuda.cu -o build/temp.linux-x86_64-3.8/csrc/megatron/scaled_masked_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(110): warning #20236-D: pragma \"diag_suppress\" is deprecated, use \"nv_diag_suppress\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(117): warning #20236-D: pragma \"diag_default\" is deprecated, use \"nv_diag_default\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(110): warning #20236-D: pragma \"diag_suppress\" is deprecated, use \"nv_diag_suppress\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(117): warning #20236-D: pragma \"diag_default\" is deprecated, use \"nv_diag_default\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(110): warning #20236-D: pragma \"diag_suppress\" is deprecated, use \"nv_diag_suppress\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(117): warning #20236-D: pragma \"diag_default\" is deprecated, use \"nv_diag_default\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(110): warning #20236-D: pragma \"diag_suppress\" is deprecated, use \"nv_diag_suppress\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(117): warning #20236-D: pragma \"diag_default\" is deprecated, use \"nv_diag_default\" instead\n",
            "\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fwrapv -O2 -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/csrc/megatron/scaled_masked_softmax.o build/temp.linux-x86_64-3.8/csrc/megatron/scaled_masked_softmax_cuda.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/scaled_masked_softmax_cuda.cpython-38-x86_64-linux-gnu.so\n",
            "building 'fused_weight_gradient_mlp_cuda' extension\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/content/apex/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/megatron/fused_weight_gradient_dense.cpp -o build/temp.linux-x86_64-3.8/csrc/megatron/fused_weight_gradient_dense.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_weight_gradient_mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/Parallel.h:140\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:13\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/megatron/fused_weight_gradient_dense.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.8/dist-packages/torch/include/ATen/ParallelOpenMP.h:83:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring #pragma omp parallel [\u001b[01;35m\u001b[K-Wunknown-pragmas\u001b[m\u001b[K]\n",
            "   83 | #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "      | \n",
            "/usr/local/cuda/bin/nvcc -I/content/apex/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/megatron/fused_weight_gradient_dense_cuda.cu -o build/temp.linux-x86_64-3.8/csrc/megatron/fused_weight_gradient_dense_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -gencode arch=compute_70,code=sm_70 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_weight_gradient_mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(110): warning #20236-D: pragma \"diag_suppress\" is deprecated, use \"nv_diag_suppress\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(117): warning #20236-D: pragma \"diag_default\" is deprecated, use \"nv_diag_default\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(110): warning #20236-D: pragma \"diag_suppress\" is deprecated, use \"nv_diag_suppress\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(117): warning #20236-D: pragma \"diag_default\" is deprecated, use \"nv_diag_default\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(110): warning #20236-D: pragma \"diag_suppress\" is deprecated, use \"nv_diag_suppress\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(117): warning #20236-D: pragma \"diag_default\" is deprecated, use \"nv_diag_default\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(110): warning #20236-D: pragma \"diag_suppress\" is deprecated, use \"nv_diag_suppress\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(117): warning #20236-D: pragma \"diag_default\" is deprecated, use \"nv_diag_default\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(110): warning #20236-D: pragma \"diag_suppress\" is deprecated, use \"nv_diag_suppress\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(117): warning #20236-D: pragma \"diag_default\" is deprecated, use \"nv_diag_default\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(110): warning #20236-D: pragma \"diag_suppress\" is deprecated, use \"nv_diag_suppress\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(117): warning #20236-D: pragma \"diag_default\" is deprecated, use \"nv_diag_default\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(110): warning #20236-D: pragma \"diag_suppress\" is deprecated, use \"nv_diag_suppress\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(117): warning #20236-D: pragma \"diag_default\" is deprecated, use \"nv_diag_default\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(110): warning #20236-D: pragma \"diag_suppress\" is deprecated, use \"nv_diag_suppress\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(117): warning #20236-D: pragma \"diag_default\" is deprecated, use \"nv_diag_default\" instead\n",
            "\n",
            "/usr/local/cuda/bin/nvcc -I/content/apex/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/megatron/fused_weight_gradient_dense_16bit_prec_cuda.cu -o build/temp.linux-x86_64-3.8/csrc/megatron/fused_weight_gradient_dense_16bit_prec_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -gencode arch=compute_70,code=sm_70 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_weight_gradient_mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(110): warning #20236-D: pragma \"diag_suppress\" is deprecated, use \"nv_diag_suppress\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(117): warning #20236-D: pragma \"diag_default\" is deprecated, use \"nv_diag_default\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(110): warning #20236-D: pragma \"diag_suppress\" is deprecated, use \"nv_diag_suppress\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(117): warning #20236-D: pragma \"diag_default\" is deprecated, use \"nv_diag_default\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(110): warning #20236-D: pragma \"diag_suppress\" is deprecated, use \"nv_diag_suppress\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(117): warning #20236-D: pragma \"diag_default\" is deprecated, use \"nv_diag_default\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(110): warning #20236-D: pragma \"diag_suppress\" is deprecated, use \"nv_diag_suppress\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(117): warning #20236-D: pragma \"diag_default\" is deprecated, use \"nv_diag_default\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(110): warning #20236-D: pragma \"diag_suppress\" is deprecated, use \"nv_diag_suppress\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(117): warning #20236-D: pragma \"diag_default\" is deprecated, use \"nv_diag_default\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(110): warning #20236-D: pragma \"diag_suppress\" is deprecated, use \"nv_diag_suppress\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(117): warning #20236-D: pragma \"diag_default\" is deprecated, use \"nv_diag_default\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(110): warning #20236-D: pragma \"diag_suppress\" is deprecated, use \"nv_diag_suppress\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(117): warning #20236-D: pragma \"diag_default\" is deprecated, use \"nv_diag_default\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(110): warning #20236-D: pragma \"diag_suppress\" is deprecated, use \"nv_diag_suppress\" instead\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/builtin_function.h(117): warning #20236-D: pragma \"diag_default\" is deprecated, use \"nv_diag_default\" instead\n",
            "\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fwrapv -O2 -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/csrc/megatron/fused_weight_gradient_dense.o build/temp.linux-x86_64-3.8/csrc/megatron/fused_weight_gradient_dense_cuda.o build/temp.linux-x86_64-3.8/csrc/megatron/fused_weight_gradient_dense_16bit_prec_cuda.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/fused_weight_gradient_mlp_cuda.cpython-38-x86_64-linux-gnu.so\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "copying build/lib.linux-x86_64-3.8/fused_layer_norm_cuda.cpython-38-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "copying build/lib.linux-x86_64-3.8/scaled_masked_softmax_cuda.cpython-38-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/apex\n",
            "creating build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib.linux-x86_64-3.8/apex/amp/_initialize.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib.linux-x86_64-3.8/apex/amp/_process_optimizer.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib.linux-x86_64-3.8/apex/amp/rnn_compat.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib.linux-x86_64-3.8/apex/amp/compat.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "creating build/bdist.linux-x86_64/egg/apex/amp/lists\n",
            "copying build/lib.linux-x86_64-3.8/apex/amp/lists/torch_overrides.py -> build/bdist.linux-x86_64/egg/apex/amp/lists\n",
            "copying build/lib.linux-x86_64-3.8/apex/amp/lists/tensor_overrides.py -> build/bdist.linux-x86_64/egg/apex/amp/lists\n",
            "copying build/lib.linux-x86_64-3.8/apex/amp/lists/__init__.py -> build/bdist.linux-x86_64/egg/apex/amp/lists\n",
            "copying build/lib.linux-x86_64-3.8/apex/amp/lists/functional_overrides.py -> build/bdist.linux-x86_64/egg/apex/amp/lists\n",
            "copying build/lib.linux-x86_64-3.8/apex/amp/wrap.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib.linux-x86_64-3.8/apex/amp/amp.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib.linux-x86_64-3.8/apex/amp/handle.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib.linux-x86_64-3.8/apex/amp/__version__.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib.linux-x86_64-3.8/apex/amp/utils.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib.linux-x86_64-3.8/apex/amp/_amp_state.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib.linux-x86_64-3.8/apex/amp/opt.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib.linux-x86_64-3.8/apex/amp/__init__.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib.linux-x86_64-3.8/apex/amp/frontend.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib.linux-x86_64-3.8/apex/amp/scaler.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/xentropy\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/xentropy/softmax_xentropy.py -> build/bdist.linux-x86_64/egg/apex/contrib/xentropy\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/xentropy/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/xentropy\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/self_multihead_attn.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/peer_memory\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/peer_memory/peer_memory.py -> build/bdist.linux-x86_64/egg/apex/contrib/peer_memory\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/peer_memory/peer_halo_exchange_module_tests.py -> build/bdist.linux-x86_64/egg/apex/contrib/peer_memory\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/peer_memory/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/peer_memory\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/bdist.linux-x86_64/egg/apex/contrib/peer_memory\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/fmha\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/fmha/fmha.py -> build/bdist.linux-x86_64/egg/apex/contrib/fmha\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/fmha/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/fmha\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/groupbn\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/groupbn/batch_norm.py -> build/bdist.linux-x86_64/egg/apex/contrib/groupbn\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/groupbn/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/groupbn\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/sparsity\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/sparsity/sparse_masklib.py -> build/bdist.linux-x86_64/egg/apex/contrib/sparsity\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_lib.py -> build/bdist.linux-x86_64/egg/apex/contrib/sparsity\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_search_kernels\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_search_kernels\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_search_kernels\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_search_kernels\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_search_kernels\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/sparsity/asp.py -> build/bdist.linux-x86_64/egg/apex/contrib/sparsity\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/sparsity/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/sparsity\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/bottleneck\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/bottleneck/halo_exchangers.py -> build/bdist.linux-x86_64/egg/apex/contrib/bottleneck\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/bottleneck/bottleneck.py -> build/bdist.linux-x86_64/egg/apex/contrib/bottleneck\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/bottleneck/bottleneck_module_test.py -> build/bdist.linux-x86_64/egg/apex/contrib/bottleneck\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/bottleneck/test.py -> build/bdist.linux-x86_64/egg/apex/contrib/bottleneck\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/bottleneck/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/bottleneck\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/layer_norm\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/layer_norm/layer_norm.py -> build/bdist.linux-x86_64/egg/apex/contrib/layer_norm\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/layer_norm/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/layer_norm\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/optimizers/distributed_fused_adam.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/optimizers/distributed_fused_lamb.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/optimizers/fused_adam.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/optimizers/fp16_optimizer.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/optimizers/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/conv_bias_relu\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/conv_bias_relu/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/conv_bias_relu\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/bdist.linux-x86_64/egg/apex/contrib/conv_bias_relu\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/focal_loss\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/focal_loss/focal_loss.py -> build/bdist.linux-x86_64/egg/apex/contrib/focal_loss\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/focal_loss/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/focal_loss\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/transducer\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/transducer/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/transducer\n",
            "copying build/lib.linux-x86_64-3.8/apex/contrib/transducer/transducer.py -> build/bdist.linux-x86_64/egg/apex/contrib/transducer\n",
            "creating build/bdist.linux-x86_64/egg/apex/normalization\n",
            "copying build/lib.linux-x86_64-3.8/apex/normalization/fused_layer_norm.py -> build/bdist.linux-x86_64/egg/apex/normalization\n",
            "copying build/lib.linux-x86_64-3.8/apex/normalization/instance_norm.py -> build/bdist.linux-x86_64/egg/apex/normalization\n",
            "copying build/lib.linux-x86_64-3.8/apex/normalization/__init__.py -> build/bdist.linux-x86_64/egg/apex/normalization\n",
            "copying build/lib.linux-x86_64-3.8/apex/_autocast_utils.py -> build/bdist.linux-x86_64/egg/apex\n",
            "creating build/bdist.linux-x86_64/egg/apex/multi_tensor_apply\n",
            "copying build/lib.linux-x86_64-3.8/apex/multi_tensor_apply/multi_tensor_apply.py -> build/bdist.linux-x86_64/egg/apex/multi_tensor_apply\n",
            "copying build/lib.linux-x86_64-3.8/apex/multi_tensor_apply/__init__.py -> build/bdist.linux-x86_64/egg/apex/multi_tensor_apply\n",
            "creating build/bdist.linux-x86_64/egg/apex/pyprof\n",
            "creating build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/__main__.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/convert.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/recurrentCell.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/embedding.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/pointwise.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/linear.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/misc.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/softmax.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/randomSample.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/conv.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/index_slice_join_mutate.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/output.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/activation.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/reduction.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/dropout.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/base.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/__init__.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/data.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/blas.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/prof.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/usage.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/loss.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/pooling.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/normalization.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/utility.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.8/apex/pyprof/prof/optim.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.8/apex/pyprof/__init__.py -> build/bdist.linux-x86_64/egg/apex/pyprof\n",
            "creating build/bdist.linux-x86_64/egg/apex/pyprof/nvtx\n",
            "copying build/lib.linux-x86_64-3.8/apex/pyprof/nvtx/nvmarker.py -> build/bdist.linux-x86_64/egg/apex/pyprof/nvtx\n",
            "copying build/lib.linux-x86_64-3.8/apex/pyprof/nvtx/__init__.py -> build/bdist.linux-x86_64/egg/apex/pyprof/nvtx\n",
            "creating build/bdist.linux-x86_64/egg/apex/pyprof/parse\n",
            "copying build/lib.linux-x86_64-3.8/apex/pyprof/parse/__main__.py -> build/bdist.linux-x86_64/egg/apex/pyprof/parse\n",
            "copying build/lib.linux-x86_64-3.8/apex/pyprof/parse/nvvp.py -> build/bdist.linux-x86_64/egg/apex/pyprof/parse\n",
            "copying build/lib.linux-x86_64-3.8/apex/pyprof/parse/db.py -> build/bdist.linux-x86_64/egg/apex/pyprof/parse\n",
            "copying build/lib.linux-x86_64-3.8/apex/pyprof/parse/parse.py -> build/bdist.linux-x86_64/egg/apex/pyprof/parse\n",
            "copying build/lib.linux-x86_64-3.8/apex/pyprof/parse/kernel.py -> build/bdist.linux-x86_64/egg/apex/pyprof/parse\n",
            "copying build/lib.linux-x86_64-3.8/apex/pyprof/parse/__init__.py -> build/bdist.linux-x86_64/egg/apex/pyprof/parse\n",
            "creating build/bdist.linux-x86_64/egg/apex/transformer\n",
            "creating build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel\n",
            "copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/p2p_communication.py -> build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel\n",
            "creating build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/schedules\n",
            "copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/schedules\n",
            "copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules/common.py -> build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/schedules\n",
            "copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/schedules\n",
            "copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules/__init__.py -> build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/schedules\n",
            "copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/schedules\n",
            "copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/utils.py -> build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel\n",
            "copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/__init__.py -> build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel\n",
            "copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/_timers.py -> build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel\n",
            "creating build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel\n",
            "copying build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel/random.py -> build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel\n",
            "copying build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel/mappings.py -> build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel\n",
            "copying build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel/cross_entropy.py -> build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel\n",
            "copying build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel/utils.py -> build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel\n",
            "copying build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel/__init__.py -> build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel\n",
            "copying build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel/data.py -> build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel\n",
            "copying build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel/memory.py -> build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel\n",
            "copying build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel/layers.py -> build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel\n",
            "creating build/bdist.linux-x86_64/egg/apex/transformer/amp\n",
            "copying build/lib.linux-x86_64-3.8/apex/transformer/amp/__init__.py -> build/bdist.linux-x86_64/egg/apex/transformer/amp\n",
            "copying build/lib.linux-x86_64-3.8/apex/transformer/amp/grad_scaler.py -> build/bdist.linux-x86_64/egg/apex/transformer/amp\n",
            "creating build/bdist.linux-x86_64/egg/apex/transformer/functional\n",
            "copying build/lib.linux-x86_64-3.8/apex/transformer/functional/fused_softmax.py -> build/bdist.linux-x86_64/egg/apex/transformer/functional\n",
            "copying build/lib.linux-x86_64-3.8/apex/transformer/functional/__init__.py -> build/bdist.linux-x86_64/egg/apex/transformer/functional\n",
            "creating build/bdist.linux-x86_64/egg/apex/transformer/_data\n",
            "copying build/lib.linux-x86_64-3.8/apex/transformer/_data/_batchsampler.py -> build/bdist.linux-x86_64/egg/apex/transformer/_data\n",
            "copying build/lib.linux-x86_64-3.8/apex/transformer/_data/__init__.py -> build/bdist.linux-x86_64/egg/apex/transformer/_data\n",
            "copying build/lib.linux-x86_64-3.8/apex/transformer/utils.py -> build/bdist.linux-x86_64/egg/apex/transformer\n",
            "copying build/lib.linux-x86_64-3.8/apex/transformer/parallel_state.py -> build/bdist.linux-x86_64/egg/apex/transformer\n",
            "creating build/bdist.linux-x86_64/egg/apex/transformer/testing\n",
            "copying build/lib.linux-x86_64-3.8/apex/transformer/testing/standalone_bert.py -> build/bdist.linux-x86_64/egg/apex/transformer/testing\n",
            "copying build/lib.linux-x86_64-3.8/apex/transformer/testing/arguments.py -> build/bdist.linux-x86_64/egg/apex/transformer/testing\n",
            "copying build/lib.linux-x86_64-3.8/apex/transformer/testing/standalone_gpt.py -> build/bdist.linux-x86_64/egg/apex/transformer/testing\n",
            "copying build/lib.linux-x86_64-3.8/apex/transformer/testing/__init__.py -> build/bdist.linux-x86_64/egg/apex/transformer/testing\n",
            "copying build/lib.linux-x86_64-3.8/apex/transformer/testing/commons.py -> build/bdist.linux-x86_64/egg/apex/transformer/testing\n",
            "copying build/lib.linux-x86_64-3.8/apex/transformer/testing/global_vars.py -> build/bdist.linux-x86_64/egg/apex/transformer/testing\n",
            "copying build/lib.linux-x86_64-3.8/apex/transformer/testing/distributed_test_base.py -> build/bdist.linux-x86_64/egg/apex/transformer/testing\n",
            "copying build/lib.linux-x86_64-3.8/apex/transformer/__init__.py -> build/bdist.linux-x86_64/egg/apex/transformer\n",
            "copying build/lib.linux-x86_64-3.8/apex/transformer/microbatches.py -> build/bdist.linux-x86_64/egg/apex/transformer\n",
            "copying build/lib.linux-x86_64-3.8/apex/transformer/log_util.py -> build/bdist.linux-x86_64/egg/apex/transformer\n",
            "copying build/lib.linux-x86_64-3.8/apex/transformer/enums.py -> build/bdist.linux-x86_64/egg/apex/transformer\n",
            "creating build/bdist.linux-x86_64/egg/apex/mlp\n",
            "copying build/lib.linux-x86_64-3.8/apex/mlp/mlp.py -> build/bdist.linux-x86_64/egg/apex/mlp\n",
            "copying build/lib.linux-x86_64-3.8/apex/mlp/__init__.py -> build/bdist.linux-x86_64/egg/apex/mlp\n",
            "creating build/bdist.linux-x86_64/egg/apex/reparameterization\n",
            "copying build/lib.linux-x86_64-3.8/apex/reparameterization/reparameterization.py -> build/bdist.linux-x86_64/egg/apex/reparameterization\n",
            "copying build/lib.linux-x86_64-3.8/apex/reparameterization/weight_norm.py -> build/bdist.linux-x86_64/egg/apex/reparameterization\n",
            "copying build/lib.linux-x86_64-3.8/apex/reparameterization/__init__.py -> build/bdist.linux-x86_64/egg/apex/reparameterization\n",
            "creating build/bdist.linux-x86_64/egg/apex/parallel\n",
            "copying build/lib.linux-x86_64-3.8/apex/parallel/sync_batchnorm.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
            "copying build/lib.linux-x86_64-3.8/apex/parallel/multiproc.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
            "copying build/lib.linux-x86_64-3.8/apex/parallel/optimized_sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
            "copying build/lib.linux-x86_64-3.8/apex/parallel/optimized_sync_batchnorm.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
            "copying build/lib.linux-x86_64-3.8/apex/parallel/sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
            "copying build/lib.linux-x86_64-3.8/apex/parallel/distributed.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
            "copying build/lib.linux-x86_64-3.8/apex/parallel/__init__.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
            "copying build/lib.linux-x86_64-3.8/apex/parallel/LARC.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
            "creating build/bdist.linux-x86_64/egg/apex/fp16_utils\n",
            "copying build/lib.linux-x86_64-3.8/apex/fp16_utils/loss_scaler.py -> build/bdist.linux-x86_64/egg/apex/fp16_utils\n",
            "copying build/lib.linux-x86_64-3.8/apex/fp16_utils/fp16util.py -> build/bdist.linux-x86_64/egg/apex/fp16_utils\n",
            "copying build/lib.linux-x86_64-3.8/apex/fp16_utils/fp16_optimizer.py -> build/bdist.linux-x86_64/egg/apex/fp16_utils\n",
            "copying build/lib.linux-x86_64-3.8/apex/fp16_utils/__init__.py -> build/bdist.linux-x86_64/egg/apex/fp16_utils\n",
            "creating build/bdist.linux-x86_64/egg/apex/RNN\n",
            "copying build/lib.linux-x86_64-3.8/apex/RNN/RNNBackend.py -> build/bdist.linux-x86_64/egg/apex/RNN\n",
            "copying build/lib.linux-x86_64-3.8/apex/RNN/models.py -> build/bdist.linux-x86_64/egg/apex/RNN\n",
            "copying build/lib.linux-x86_64-3.8/apex/RNN/cells.py -> build/bdist.linux-x86_64/egg/apex/RNN\n",
            "copying build/lib.linux-x86_64-3.8/apex/RNN/__init__.py -> build/bdist.linux-x86_64/egg/apex/RNN\n",
            "creating build/bdist.linux-x86_64/egg/apex/optimizers\n",
            "copying build/lib.linux-x86_64-3.8/apex/optimizers/fused_adam.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n",
            "copying build/lib.linux-x86_64-3.8/apex/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n",
            "copying build/lib.linux-x86_64-3.8/apex/optimizers/fused_novograd.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n",
            "copying build/lib.linux-x86_64-3.8/apex/optimizers/fused_mixed_precision_lamb.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n",
            "copying build/lib.linux-x86_64-3.8/apex/optimizers/fused_adagrad.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n",
            "copying build/lib.linux-x86_64-3.8/apex/optimizers/__init__.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n",
            "copying build/lib.linux-x86_64-3.8/apex/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n",
            "copying build/lib.linux-x86_64-3.8/apex/__init__.py -> build/bdist.linux-x86_64/egg/apex\n",
            "creating build/bdist.linux-x86_64/egg/apex/fused_dense\n",
            "copying build/lib.linux-x86_64-3.8/apex/fused_dense/fused_dense.py -> build/bdist.linux-x86_64/egg/apex/fused_dense\n",
            "copying build/lib.linux-x86_64-3.8/apex/fused_dense/__init__.py -> build/bdist.linux-x86_64/egg/apex/fused_dense\n",
            "copying build/lib.linux-x86_64-3.8/fused_dense_cuda.cpython-38-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "copying build/lib.linux-x86_64-3.8/syncbn.cpython-38-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "copying build/lib.linux-x86_64-3.8/amp_C.cpython-38-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "copying build/lib.linux-x86_64-3.8/scaled_upper_triang_masked_softmax_cuda.cpython-38-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "copying build/lib.linux-x86_64-3.8/mlp_cuda.cpython-38-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "copying build/lib.linux-x86_64-3.8/fused_weight_gradient_mlp_cuda.cpython-38-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "copying build/lib.linux-x86_64-3.8/apex_C.cpython-38-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/_initialize.py to _initialize.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/_process_optimizer.py to _process_optimizer.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/rnn_compat.py to rnn_compat.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/compat.py to compat.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/lists/torch_overrides.py to torch_overrides.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/lists/tensor_overrides.py to tensor_overrides.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/lists/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/lists/functional_overrides.py to functional_overrides.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/wrap.py to wrap.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/amp.py to amp.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/handle.py to handle.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/__version__.py to __version__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/utils.py to utils.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/_amp_state.py to _amp_state.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/opt.py to opt.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/frontend.py to frontend.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/scaler.py to scaler.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/xentropy/softmax_xentropy.py to softmax_xentropy.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/xentropy/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/self_multihead_attn.py to self_multihead_attn.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/mask_softmax_dropout_func.py to mask_softmax_dropout_func.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py to fast_encdec_multihead_attn_norm_add_func.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/encdec_multihead_attn_func.py to encdec_multihead_attn_func.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py to fast_encdec_multihead_attn_func.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py to fast_self_multihead_attn_func.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/self_multihead_attn_func.py to self_multihead_attn_func.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py to fast_self_multihead_attn_norm_add_func.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/encdec_multihead_attn.py to encdec_multihead_attn.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/peer_memory/peer_memory.py to peer_memory.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/peer_memory/peer_halo_exchange_module_tests.py to peer_halo_exchange_module_tests.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/peer_memory/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/peer_memory/peer_halo_exchanger_1d.py to peer_halo_exchanger_1d.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/fmha/fmha.py to fmha.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/fmha/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/groupbn/batch_norm.py to batch_norm.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/groupbn/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/sparsity/sparse_masklib.py to sparse_masklib.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_lib.py to permutation_lib.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py to exhaustive_search.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py to permutation_utilities.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py to call_permutation_search_kernels.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/sparsity/permutation_search_kernels/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/sparsity/asp.py to asp.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/sparsity/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/bottleneck/halo_exchangers.py to halo_exchangers.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/bottleneck/bottleneck.py to bottleneck.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/bottleneck/bottleneck_module_test.py to bottleneck_module_test.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/bottleneck/test.py to test.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/bottleneck/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/layer_norm/layer_norm.py to layer_norm.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/layer_norm/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/distributed_fused_adam.py to distributed_fused_adam.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/distributed_fused_lamb.py to distributed_fused_lamb.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/fused_adam.py to fused_adam.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/fused_lamb.py to fused_lamb.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/fp16_optimizer.py to fp16_optimizer.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/distributed_fused_adam_v2.py to distributed_fused_adam_v2.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/distributed_fused_adam_v3.py to distributed_fused_adam_v3.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/fused_sgd.py to fused_sgd.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/conv_bias_relu/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/conv_bias_relu/conv_bias_relu.py to conv_bias_relu.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/focal_loss/focal_loss.py to focal_loss.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/focal_loss/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/transducer/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/transducer/transducer.py to transducer.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/normalization/fused_layer_norm.py to fused_layer_norm.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/normalization/instance_norm.py to instance_norm.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/normalization/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/_autocast_utils.py to _autocast_utils.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/multi_tensor_apply/multi_tensor_apply.py to multi_tensor_apply.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/multi_tensor_apply/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/__main__.py to __main__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/convert.py to convert.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/recurrentCell.py to recurrentCell.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/embedding.py to embedding.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/pointwise.py to pointwise.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/linear.py to linear.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/misc.py to misc.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/softmax.py to softmax.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/randomSample.py to randomSample.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/conv.py to conv.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/index_slice_join_mutate.py to index_slice_join_mutate.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/output.py to output.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/activation.py to activation.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/reduction.py to reduction.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/dropout.py to dropout.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/base.py to base.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/data.py to data.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/blas.py to blas.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/prof.py to prof.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/usage.py to usage.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/loss.py to loss.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/pooling.py to pooling.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/normalization.py to normalization.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/utility.py to utility.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/optim.py to optim.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/nvtx/nvmarker.py to nvmarker.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/nvtx/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/parse/__main__.py to __main__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/parse/nvvp.py to nvvp.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/parse/db.py to db.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/parse/parse.py to parse.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/parse/kernel.py to kernel.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/parse/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/p2p_communication.py to p2p_communication.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py to fwd_bwd_pipelining_without_interleaving.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/schedules/common.py to common.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py to fwd_bwd_pipelining_with_interleaving.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/schedules/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py to fwd_bwd_no_pipelining.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/utils.py to utils.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/pipeline_parallel/_timers.py to _timers.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel/random.py to random.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel/mappings.py to mappings.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel/cross_entropy.py to cross_entropy.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel/utils.py to utils.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel/data.py to data.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel/memory.py to memory.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/tensor_parallel/layers.py to layers.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/amp/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/amp/grad_scaler.py to grad_scaler.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/functional/fused_softmax.py to fused_softmax.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/functional/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/_data/_batchsampler.py to _batchsampler.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/_data/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/utils.py to utils.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/parallel_state.py to parallel_state.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/testing/standalone_bert.py to standalone_bert.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/testing/arguments.py to arguments.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/testing/standalone_gpt.py to standalone_gpt.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/testing/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/testing/commons.py to commons.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/testing/global_vars.py to global_vars.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/testing/distributed_test_base.py to distributed_test_base.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/microbatches.py to microbatches.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/log_util.py to log_util.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/transformer/enums.py to enums.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/mlp/mlp.py to mlp.cpython-38.pyc\n",
            "build/bdist.linux-x86_64/egg/apex/mlp/mlp.py:40: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if activation is 'none':\n",
            "build/bdist.linux-x86_64/egg/apex/mlp/mlp.py:42: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  elif activation is 'relu':\n",
            "build/bdist.linux-x86_64/egg/apex/mlp/mlp.py:44: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  elif activation is 'sigmoid':\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/mlp/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/reparameterization/reparameterization.py to reparameterization.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/reparameterization/weight_norm.py to weight_norm.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/reparameterization/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/sync_batchnorm.py to sync_batchnorm.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/multiproc.py to multiproc.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/optimized_sync_batchnorm_kernel.py to optimized_sync_batchnorm_kernel.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/optimized_sync_batchnorm.py to optimized_sync_batchnorm.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/sync_batchnorm_kernel.py to sync_batchnorm_kernel.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/distributed.py to distributed.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/LARC.py to LARC.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/fp16_utils/loss_scaler.py to loss_scaler.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/fp16_utils/fp16util.py to fp16util.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/fp16_utils/fp16_optimizer.py to fp16_optimizer.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/fp16_utils/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/RNN/RNNBackend.py to RNNBackend.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/RNN/models.py to models.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/RNN/cells.py to cells.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/RNN/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/fused_adam.py to fused_adam.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/fused_lamb.py to fused_lamb.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/fused_novograd.py to fused_novograd.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/fused_mixed_precision_lamb.py to fused_mixed_precision_lamb.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/fused_adagrad.py to fused_adagrad.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/fused_sgd.py to fused_sgd.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/fused_dense/fused_dense.py to fused_dense.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/fused_dense/__init__.py to __init__.cpython-38.pyc\n",
            "creating stub loader for apex_C.cpython-38-x86_64-linux-gnu.so\n",
            "creating stub loader for amp_C.cpython-38-x86_64-linux-gnu.so\n",
            "creating stub loader for syncbn.cpython-38-x86_64-linux-gnu.so\n",
            "creating stub loader for fused_layer_norm_cuda.cpython-38-x86_64-linux-gnu.so\n",
            "creating stub loader for mlp_cuda.cpython-38-x86_64-linux-gnu.so\n",
            "creating stub loader for fused_dense_cuda.cpython-38-x86_64-linux-gnu.so\n",
            "creating stub loader for scaled_upper_triang_masked_softmax_cuda.cpython-38-x86_64-linux-gnu.so\n",
            "creating stub loader for scaled_masked_softmax_cuda.cpython-38-x86_64-linux-gnu.so\n",
            "creating stub loader for fused_weight_gradient_mlp_cuda.cpython-38-x86_64-linux-gnu.so\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex_C.py to apex_C.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/amp_C.py to amp_C.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/syncbn.py to syncbn.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/fused_layer_norm_cuda.py to fused_layer_norm_cuda.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mlp_cuda.py to mlp_cuda.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/fused_dense_cuda.py to fused_dense_cuda.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/scaled_upper_triang_masked_softmax_cuda.py to scaled_upper_triang_masked_softmax_cuda.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/scaled_masked_softmax_cuda.py to scaled_masked_softmax_cuda.cpython-38.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/fused_weight_gradient_mlp_cuda.py to fused_weight_gradient_mlp_cuda.cpython-38.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying apex.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying apex.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying apex.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying apex.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "__pycache__.amp_C.cpython-38: module references __file__\n",
            "__pycache__.apex_C.cpython-38: module references __file__\n",
            "__pycache__.fused_dense_cuda.cpython-38: module references __file__\n",
            "__pycache__.fused_layer_norm_cuda.cpython-38: module references __file__\n",
            "__pycache__.fused_weight_gradient_mlp_cuda.cpython-38: module references __file__\n",
            "__pycache__.mlp_cuda.cpython-38: module references __file__\n",
            "__pycache__.scaled_masked_softmax_cuda.cpython-38: module references __file__\n",
            "__pycache__.scaled_upper_triang_masked_softmax_cuda.cpython-38: module references __file__\n",
            "__pycache__.syncbn.cpython-38: module references __file__\n",
            "apex.pyprof.nvtx.__pycache__.nvmarker.cpython-38: module references __file__\n",
            "apex.pyprof.nvtx.__pycache__.nvmarker.cpython-38: module references __path__\n",
            "creating dist\n",
            "creating 'dist/apex-0.1-py3.8-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing apex-0.1-py3.8-linux-x86_64.egg\n",
            "creating /usr/local/lib/python3.8/dist-packages/apex-0.1-py3.8-linux-x86_64.egg\n",
            "Extracting apex-0.1-py3.8-linux-x86_64.egg to /usr/local/lib/python3.8/dist-packages\n",
            "/usr/local/lib/python3.8/dist-packages/apex-0.1-py3.8-linux-x86_64.egg/apex/mlp/mlp.py:40: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if activation is 'none':\n",
            "/usr/local/lib/python3.8/dist-packages/apex-0.1-py3.8-linux-x86_64.egg/apex/mlp/mlp.py:42: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  elif activation is 'relu':\n",
            "/usr/local/lib/python3.8/dist-packages/apex-0.1-py3.8-linux-x86_64.egg/apex/mlp/mlp.py:44: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  elif activation is 'sigmoid':\n",
            "Adding apex 0.1 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.8/dist-packages/apex-0.1-py3.8-linux-x86_64.egg\n",
            "Processing dependencies for apex==0.1\n",
            "Finished processing dependencies for apex==0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have setup all the required dependancies, lets quicky check if all packages are there"
      ],
      "metadata": {
        "id": "HgQD7Nyiyl1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlBLvEW_ye12",
        "outputId": "f067e7be-4e3e-4bb5-fc3c-3922d42c4876"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Package             Version             \n",
            "------------------- --------------------\n",
            "apex                0.1                 \n",
            "certifi             2023.5.7            \n",
            "charset-normalizer  3.1.0               \n",
            "click               8.1.3               \n",
            "dbus-python         1.2.16              \n",
            "filelock            3.12.0              \n",
            "idna                3.4                 \n",
            "joblib              1.2.0               \n",
            "Levenshtein         0.20.9              \n",
            "nltk                3.6.2               \n",
            "numpy               1.19.2              \n",
            "packaging           23.1                \n",
            "pip                 20.0.2              \n",
            "protobuf            3.20.3              \n",
            "PyGObject           3.36.0              \n",
            "python-apt          2.0.1+ubuntu0.20.4.1\n",
            "python-Levenshtein  0.20.9              \n",
            "rapidfuzz           2.15.1              \n",
            "regex               2023.5.5            \n",
            "requests            2.31.0              \n",
            "requests-unixsocket 0.2.0               \n",
            "sacremoses          0.0.53              \n",
            "scikit-learn        1.2.2               \n",
            "scipy               1.10.1              \n",
            "sentencepiece       0.1.99              \n",
            "seqeval             1.2.2               \n",
            "setuptools          45.2.0              \n",
            "six                 1.16.0              \n",
            "tensorboardX        2.6                 \n",
            "threadpoolctl       3.1.0               \n",
            "tokenizers          0.7.0               \n",
            "torch               1.8.1+cu111         \n",
            "tqdm                4.49.0              \n",
            "transformers        2.10.0              \n",
            "typing-extensions   4.6.2               \n",
            "urllib3             2.0.2               \n",
            "wheel               0.34.2              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before starting training of LASER on FUNSD, We need to change some config in `/content/LASER/shell_scripts/run_few_shot_FUNSD.sh`  \n",
        "\n",
        "\n",
        "\n",
        "- On `LINE 4` change the `PROJECT` path to `/content/LASER`\n",
        "- On `LINE 43` change `--per_gpu_train_batch_size` to `4`, otherwise we'd get GPU Memory exhuasted\n"
      ],
      "metadata": {
        "id": "kr21o9aWy0xq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's It!  \n",
        "Now just run the below cell and watch the LASER model train on FUNSD dataset.   \n",
        "\n",
        "It'll loop through different size `(1 3 5 7)` for few-shot approach.  \n",
        "\n",
        "---\n",
        "\n",
        "**NOTE**: Each training round will take large amount of time to train on colab GPU, so if you don't wan't to go through various possible values for few shots with different seeds. \n",
        "\n",
        "For that change `LINE 7` to have no of samples on which you want to train  \n",
        "Ex. \n",
        "\n",
        "```bash\n",
        "5  cd $PROJECT\n",
        "6  \n",
        "7  for sz in 1\n",
        "8  do\n",
        "9     for sd in 1\n",
        "10    do\n",
        "11        echo ====FUNSD sz: $sz, sd: $sd====\n",
        "```\n",
        "\n",
        "\n",
        "Notice on `LINE 11` we `1 3 5 7` changed to `1`, it can be any value in range `[1, 10]`.   \n",
        "Similarly, we also changed `1 2 3 4 5 6` to `1` on `LINE 9`, it's just a seed value, you can keep it anything in range `[1, 10]`  \n",
        "\n",
        "For more details and understanding see file `/content/LASER/data_utils/funsd_few_shot_info.json`\n",
        "\n",
        "---\n",
        "\n",
        "Lets start the training!\n"
      ],
      "metadata": {
        "id": "nOa5UB40z7jE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LASER && \\\n",
        "sh shell_scripts/run_few_shot_FUNSD.sh 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ap8PnCpyxit",
        "outputId": "f00240e9-4f3e-4ab5-e4ec-6adcbbeb3986"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====FUNSD sz: 1, sd: 1====\n",
            "==TRAIN==\n",
            "/usr/local/lib/python3.8/dist-packages/scipy/__init__.py:143: UserWarning: A NumPy version >=1.19.5 and <1.27.0 is required for this version of SciPy (detected version 1.19.2)\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
            "05/30/2023 13:33:08 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: True\n",
            "05/30/2023 13:33:08 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir=None, cached_train_features_file=None, config_name='/content/LASER/weights/layoutreader/config.json', device=device(type='cuda'), do_lower_case=True, extra_token_without_layout=False, few_shot_info='/content/LASER/data_utils/funsd_few_shot_info.json', few_shot_seed=1, few_shot_size=1, fp16=True, fp16_opt_level='O2', gradient_accumulation_steps=1, keep_prob=0.1, label_smoothing=0.1, learning_rate=5e-05, local_rank=-1, log_dir=None, logging_steps=100, max_grad_norm=1.0, max_source_seq_length=513, max_target_seq_length=511, meta_file='/content/LASER/data/funsd/meta.json', model_name_or_path='/content/LASER/weights/layoutreader/pytorch_model.bin', model_type='layoutlm', n_gpu=1, no_cuda=False, num_training_epochs=200, num_training_steps=-1, num_warmup_rate=0.0, num_warmup_steps=-1, output_dir='/content/LASER/outputs/funsd/funsd-sz1-sd1/train_result', per_gpu_train_batch_size=4, pure_text_dataset=False, random_prob=0.1, save_last_step=True, save_steps=-1, seed=42, server_ip='', server_port='', tokenizer_name='/content/LASER/weights/special_bert_base_uncased_tokenizer', train_file='/content/LASER/data/funsd/train-text-s2s.jsons', train_folder=None, weight_decay=0.01)\n",
            "05/30/2023 13:33:08 - INFO - transformers.configuration_utils -   loading configuration file /content/LASER/weights/layoutreader/config.json\n",
            "05/30/2023 13:33:08 - INFO - transformers.configuration_utils -   Model config LayoutlmConfig {\n",
            "  \"architectures\": [\n",
            "    \"LayoutlmForSequenceToSequence\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"base_model_type\": \"layoutlm\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label_smoothing\": 0.1,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_2d_position_embeddings\": 1024,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"max_source_length\": 513,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"source_type_id\": 0,\n",
            "  \"target_type_id\": 1,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522,\n",
            "  \"vocab_size_or_config_json_file\": 30522\n",
            "}\n",
            "\n",
            "05/30/2023 13:33:08 - INFO - __main__ -   Model config for seq2seq: BertForSeq2SeqConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"base_model_type\": \"layoutlm\",\n",
            "  \"extra_token_with_layout\": true,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label_smoothing\": 0.1,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_2d_position_embeddings\": 1024,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"max_source_length\": 513,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"source_type_id\": 0,\n",
            "  \"target_type_id\": 1,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522,\n",
            "  \"vocab_size_or_config_json_file\": 30522\n",
            "}\n",
            "\n",
            "05/30/2023 13:33:08 - INFO - transformers.tokenization_utils -   Model name '/content/LASER/weights/special_bert_base_uncased_tokenizer' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '/content/LASER/weights/special_bert_base_uncased_tokenizer' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "05/30/2023 13:33:08 - INFO - transformers.tokenization_utils -   Didn't find file /content/LASER/weights/special_bert_base_uncased_tokenizer/added_tokens.json. We won't load it.\n",
            "05/30/2023 13:33:08 - INFO - transformers.tokenization_utils -   Didn't find file /content/LASER/weights/special_bert_base_uncased_tokenizer/special_tokens_map.json. We won't load it.\n",
            "05/30/2023 13:33:08 - INFO - transformers.tokenization_utils -   loading file /content/LASER/weights/special_bert_base_uncased_tokenizer/vocab.txt\n",
            "05/30/2023 13:33:08 - INFO - transformers.tokenization_utils -   loading file None\n",
            "05/30/2023 13:33:08 - INFO - transformers.tokenization_utils -   loading file None\n",
            "05/30/2023 13:33:08 - INFO - transformers.tokenization_utils -   loading file /content/LASER/weights/special_bert_base_uncased_tokenizer/tokenizer_config.json\n",
            "05/30/2023 13:33:08 - INFO - transformers.tokenization_utils -   Adding <begin> to the vocabulary\n",
            "05/30/2023 13:33:08 - INFO - transformers.tokenization_utils -   Adding <end> to the vocabulary\n",
            "05/30/2023 13:33:08 - INFO - transformers.tokenization_utils -   Adding <tag_end> to the vocabulary\n",
            "05/30/2023 13:33:08 - INFO - transformers.tokenization_utils -   Assigning ['<BEGIN>', '<END>', '<TAG_END>'] to the additional_special_tokens key of the tokenizer\n",
            "05/30/2023 13:33:08 - INFO - transformers.modeling_utils -   loading weights file /content/LASER/weights/layoutreader/pytorch_model.bin\n",
            "05/30/2023 13:33:10 - INFO - transformers.modeling_utils -   Weights of LayoutlmForSequenceToSequence not initialized from pretrained model: ['extra_token_cls.predictions.bias', 'extra_token_cls.predictions.transform.dense.weight', 'extra_token_cls.predictions.transform.dense.bias', 'extra_token_cls.predictions.transform.LayerNorm.weight', 'extra_token_cls.predictions.transform.LayerNorm.bias', 'classifier.weight', 'classifier.bias', 'crit_mask_lm_smoothed.one_hot']\n",
            "05/30/2023 13:33:10 - INFO - s2s_ft.utils -   Creating features from dataset at /content/LASER/data/funsd/train-text-s2s.jsons\n",
            "05/30/2023 13:33:10 - INFO - s2s_ft.utils -   Start loading /content/LASER/data/funsd/train-text-s2s.jsons\n",
            "100% 21/21 [00:00<00:00, 138.69it/s]\n",
            "05/30/2023 13:33:10 - INFO - s2s_ft.utils -   Saving features into cached file /content/LASER/outputs/funsd/funsd-sz1-sd1/train_result/cached_features_for_training.pt\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "05/30/2023 13:33:15 - INFO - __main__ -   Mode = LayoutlmForSequenceToSequence(\n",
            "  (bert): LayoutlmModel(\n",
            "    (embeddings): LayoutlmEmbeddings(\n",
            "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(1024, 768)\n",
            "      (x_position_embeddings): Embedding(1024, 768)\n",
            "      (y_position_embeddings): Embedding(1024, 768)\n",
            "      (h_position_embeddings): Embedding(1024, 768)\n",
            "      (w_position_embeddings): Embedding(1024, 768)\n",
            "      (token_type_embeddings): Embedding(2, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (cls): LayoutlmSPOnlyMLMHead(\n",
            "    (predictions): LayoutlmSPLMPredictionHead(\n",
            "      (transform): BertPredictionHeadTransform(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (extra_token_cls): LayoutlmSPOnlyMLMHead(\n",
            "    (predictions): LayoutlmSPLMPredictionHead(\n",
            "      (transform): BertPredictionHeadTransform(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
            "  (log_softmax): LogSoftmax(dim=None)\n",
            "  (crit_mask_lm_smoothed): LabelSmoothingLoss()\n",
            ")\n",
            "05/30/2023 13:33:15 - INFO - __main__ -     ***** Running training *****  *\n",
            "05/30/2023 13:33:15 - INFO - __main__ -     Num examples = 21\n",
            "05/30/2023 13:33:15 - INFO - __main__ -     Num Epochs = 200.00\n",
            "05/30/2023 13:33:15 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
            "05/30/2023 13:33:15 - INFO - __main__ -     Batch size per node = 4\n",
            "05/30/2023 13:33:15 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "05/30/2023 13:33:15 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "05/30/2023 13:33:15 - INFO - __main__ -     Total optimization steps = 1050\n",
            "Iter (loss=9.591) lr=0.0000500:   0% 0/1050 [00:00<?, ?it/s] /usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:122: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Seems like `optimizer.step()` has been overridden after learning rate scheduler \"\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Iter (loss=8.963) lr=0.0000500:   0% 1/1050 [00:01<22:21,  1.28s/it]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "Iter (loss=9.576) lr=0.0000499:   0% 2/1050 [00:02<19:40,  1.13s/it]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "Iter (loss=8.387) lr=0.0000499:   0% 3/1050 [00:03<17:47,  1.02s/it]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "Iter (loss=10.198) lr=0.0000498:   0% 4/1050 [00:03<16:28,  1.06it/s]Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
            "Iter (loss=9.399) lr=0.0000498:   0% 5/1050 [00:04<15:31,  1.12it/s] Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0\n",
            "Iter (loss=0.058) lr=0.0000000: 100% 1050/1050 [15:04<00:00,  1.16it/s]\n",
            "05/30/2023 13:48:19 - INFO - transformers.configuration_utils -   Configuration saved in /content/LASER/outputs/funsd/funsd-sz1-sd1/train_result/ckpt-last/config.json\n",
            "05/30/2023 13:48:25 - INFO - transformers.modeling_utils -   Model weights saved in /content/LASER/outputs/funsd/funsd-sz1-sd1/train_result/ckpt-last/pytorch_model.bin\n",
            "05/30/2023 13:48:25 - INFO - __main__ -   Saving model checkpoint last into /content/LASER/outputs/funsd/funsd-sz1-sd1/train_result/ckpt-last\n",
            "==DECODE==\n",
            "/usr/local/lib/python3.8/dist-packages/scipy/__init__.py:143: UserWarning: A NumPy version >=1.19.5 and <1.27.0 is required for this version of SciPy (detected version 1.19.2)\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
            "05/30/2023 13:48:27 - INFO - transformers.tokenization_utils -   Model name '/content/LASER/weights/special_bert_base_uncased_tokenizer' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '/content/LASER/weights/special_bert_base_uncased_tokenizer' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "05/30/2023 13:48:27 - INFO - transformers.tokenization_utils -   Didn't find file /content/LASER/weights/special_bert_base_uncased_tokenizer/added_tokens.json. We won't load it.\n",
            "05/30/2023 13:48:27 - INFO - transformers.tokenization_utils -   Didn't find file /content/LASER/weights/special_bert_base_uncased_tokenizer/special_tokens_map.json. We won't load it.\n",
            "05/30/2023 13:48:27 - INFO - transformers.tokenization_utils -   loading file /content/LASER/weights/special_bert_base_uncased_tokenizer/vocab.txt\n",
            "05/30/2023 13:48:27 - INFO - transformers.tokenization_utils -   loading file None\n",
            "05/30/2023 13:48:27 - INFO - transformers.tokenization_utils -   loading file None\n",
            "05/30/2023 13:48:27 - INFO - transformers.tokenization_utils -   loading file /content/LASER/weights/special_bert_base_uncased_tokenizer/tokenizer_config.json\n",
            "05/30/2023 13:48:27 - INFO - transformers.tokenization_utils -   Adding <begin> to the vocabulary\n",
            "05/30/2023 13:48:27 - INFO - transformers.tokenization_utils -   Adding <end> to the vocabulary\n",
            "05/30/2023 13:48:27 - INFO - transformers.tokenization_utils -   Adding <tag_end> to the vocabulary\n",
            "05/30/2023 13:48:27 - INFO - transformers.tokenization_utils -   Assigning ['<BEGIN>', '<END>', '<TAG_END>'] to the additional_special_tokens key of the tokenizer\n",
            "05/30/2023 13:48:27 - INFO - __main__ -   Read decoding config from: /content/LASER/outputs/funsd/funsd-sz1-sd1/train_result/ckpt-last/config.json\n",
            "05/30/2023 13:48:27 - INFO - __main__ -   ***** Recover model: /content/LASER/outputs/funsd/funsd-sz1-sd1/train_result/ckpt-last *****\n",
            "05/30/2023 13:48:27 - INFO - s2s_ft.modeling_decoding -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"LayoutlmForSequenceToSequence\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"base_model_type\": \"layoutlm\",\n",
            "  \"extra_token_with_layout\": true,\n",
            "  \"ffn_type\": 0,\n",
            "  \"fp32_embedding\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label_smoothing\": 0.1,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_2d_position_embeddings\": 1024,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"max_source_length\": 513,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"new_pos_ids\": false,\n",
            "  \"no_segment_embedding\": false,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_qkv\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relax_projection\": 0,\n",
            "  \"seg_emb\": false,\n",
            "  \"source_type_id\": 0,\n",
            "  \"target_type_id\": 1,\n",
            "  \"task_idx\": null,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522,\n",
            "  \"vocab_size_or_config_json_file\": 30522\n",
            "}\n",
            "\n",
            "05/30/2023 13:48:30 - INFO - s2s_ft.modeling_decoding -   Weights of LayoutlmForSeq2SeqDecoder not initialized from pretrained model: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'extra_token_cls.seq_relationship.weight', 'extra_token_cls.seq_relationship.bias']\n",
            "05/30/2023 13:48:30 - INFO - s2s_ft.modeling_decoding -   Weights from pretrained model not used in LayoutlmForSeq2SeqDecoder: ['crit_mask_lm_smoothed.one_hot']\n",
            "05/30/2023 13:48:34 - INFO - s2s_ft.utils -   Creating features from dataset at /content/LASER/data/funsd/test-text-s2s.jsons\n",
            "05/30/2023 13:48:34 - INFO - s2s_ft.utils -   Start loading /content/LASER/data/funsd/test-text-s2s.jsons\n",
            "100% 127/127 [00:01<00:00, 94.46it/s]\n",
            "100% 13/13 [14:36<00:00, 67.39s/it]\n",
            "==EVAL==\n",
            "/usr/local/lib/python3.8/dist-packages/scipy/__init__.py:143: UserWarning: A NumPy version >=1.19.5 and <1.27.0 is required for this version of SciPy (detected version 1.19.2)\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
            "pre: 0.3376297887576083\n",
            "rec: 0.471971971971972\n",
            "f-1: 0.39365476935921523\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      answer       0.25      0.56      0.35       809\n",
            "      header       0.27      0.03      0.05       119\n",
            "    question       0.49      0.46      0.47      1070\n",
            "\n",
            "   micro avg       0.34      0.47      0.39      1998\n",
            "   macro avg       0.34      0.35      0.29      1998\n",
            "weighted avg       0.38      0.47      0.40      1998\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2CMqNIhY3AjC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}